%! Author = andrei.olar@ubbcluj.ro
%! Date = 03.09.2023

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{cite}
\usepackage{array}
\usepackage{graphicx}
\graphicspath{ {./img/} }

% Document
\begin{document}
    \theoremstyle{definition}
    \newtheorem{defn}{Definition}[section]
    
    \section{Abstract}\label{sec:abstract}


    \section{Introduction}\label{sec:introduction}
    Entity resolution is the task of finding out whether two pieces of
    information refer to the same real-world item or not.
    There's more restrictive definitions of entity resolution as the task of
    identifying and linking representations of data from two or more
    sources\cite{Qia17}.
    We share the opinion that identifying and linking data consitutes a more
    specialized process\cite{Tal11}.
    The task of gathering information about a generic pound of potatoes across
    various markets is still an entity resolution task, in our view.
    
    Probably because of its generic nature, entity resolution also goes by many
    different names such as: record linkage, data deduplication, merge-purge,
    named entity recognition, entity alignment or entity
    matching\cite{Tal11,fever2009}.

    Entity resolution has many practical applications ranging from linking
    medical records to doing background checks on persons of interest to 
    identifying plagiarism.
    As an aside, combining information from different media (sound, images,
    motion, smell, etc) is also a form of entity resolution and opens up many
    avenues into its future as an area of study.

    The glue that binds all of the above examples has at least the following
    ingredients:
    \begin{itemize}
        \item\textit{a concern for a real-world entity}, for if the task were
        not about a something from our world, the resolution would not have a
        clear goal;
        \item\textit{no implication as to the used method}, because if we
        stipulated a certain way to perform entity resolution, we might not be
        able to;
        \item\textit{the representation of information is computer friendly},
        because if it weren't, it might not be possible to study the task by
        means of computer science.
    \end{itemize}

    We must clarify regarding the concern for a real-world entity that entity
    resolution tasks do not deal with real-world entities directly but with
    information about them\cite{Tal11}.
    In other words, entity resolution tasks typically don't know about the
    real-world objects or entities they are meant to resolve\cite{Chen09}.
    Naturally, entity resolution tasks will not perform perfectly from a
    qualitative standpoint.
    We are interested to find common ground for measuring entity resolution
    performance.
    
    Historically, entity resolution performance was measured in many ways, some
    of which (true positive rate, false positive rate) have been contested for
    certain use cases\cite{Goga2015}.
    Our work means to standardize qualitative entity resolution quality
    measurement across the board.
    We base our methodology on the mathematical models that underpin entity
    resolution tasks.

    The first sections of this paper go over the contributions it makes and the
    work related to this paper.
    Afterwards we start our journey towards the main subject by introducing some
    basic entity resolution terminology which is used throughout the paper.
    Following that, we describe the scientific problem of comparing the
    performance of entity resolution tasks.
    Next we go through three classical mathematical models and show how each of
    them influences entity resolution evaluation.
    After the conceptual framework is ready, we provide experimental data to see
    how the concepts map to practice.
    In the closing sections, we provide some insights that we are now able to 
    glean from the results, draw some conclusions and finally hint towards
    future work on this topic.

    \section{Contributions}\label{sec:contributions}

    This paper makes the following contributions:

    \begin{itemize}
        \item materializes the connection between mathematical model and entity
        resolution task
        \item provides a generic framework for measuring the qualitative
        performance of entity resolution tasks
        \item introduces a software system that uses this framework
    \end{itemize}

    \section{Related Work}\label{sec:related}
    
    Because of the generality of the topic, there is much existing work that
    relates in some way or another to this paper.
    However, three categories of papers share the most concerns with this one:
    entity resolution system presentations, syntheses of theoretical models for
    entity resolution and syntheses of evaluation metrics for entity resolution.

    The papers that introduce entity resolution systems relate to this one
    through the shared goal of generalizing how entity resolution tasks are run
    and, therefore, evaluated.
    Most of the systems referenced here provide at least a few generic metrics
    to evaluate entity resolution performance.
    They 

    At the dawn of the 21st century, FEBRL\cite{febrl2002} emerged as one of the
    first extensible systems to tackle measuring entity resolution performance.
    It was succeeded by FEVER\cite{fever2009} and OYSTER\cite{oyster2012}. 
    These systems have all paved the way to generalizing our understanding about
    entity resolution and to ever more improvements in the quality of entity
    resolution tasks.
    Later on, systems like `Papers with Code'\cite{papwithcode2019} brought in
    a social dimension to how we compare the qualitative performance of entity
    resolution tasks.
    Newer systems resemble frameworks more than they resemble
    applications\cite{magellan2020,jedai2017}.
    These systems are open-source and encourage social collaboration around
    entity resolution, too.

    
    There exist numerous syntheses on the theoretical models for entity
    resolution\cite{fs1969,Ben2009Swoosh,Tal11}.
    Some of the available models have been compared and
    explained\cite{Tal11,tal2013}.

    Ways in which entity resolution tasks are similar even though they seem
    to have nothing in common have been discussed in papers that deal with
    certain steps of the entity resolution process\cite{Pap19,Chen09}.

    \section{Terminology}\label{sec:terminology}

    Given that we have just reserved the term ``entity'' for real-world items,
    we have to come up with new terms for the concepts used in the context of
    entity resolution.
    We start by saying that entity resolution operates on information sources.

    \begin{defn}
        An \textit{information source} represents a sequence of decoded messages
        that originate somewhere and can be processed by the entity resolution
        task which reads them over a communication channel.
    \end{defn}

    The terms `decoded', `message' and `communication channel' refer to concepts
    from the field of information theory\cite{ash2012it}.
    
    An entity resolution task can operate on one or multiple information
    sources.
    Information sources may be bounded (such as files or databases) or unbounded
    (such as streams).
    
    In the context of the above definition, the entity resolution task is the
    computer program doing the processing.
    What sets entity resolution programs apart is the notions they use to
    process information from a particular source.

    \begin{defn}
        An \textit{attribute} is information about an entity that has a certain
        meaning in a context given by a frame of reference and certain rules of
        interpretation.
    \end{defn}

    Attributes by themselves are not enough to describe an entity.
    For example, `red' fully describes an entity only if we are looking for
    colors.
    If we're looking for fruit, `red' is only one of several attributes that
    help describe fruit.
    For example we might be interested in `sour' or `sweet' as well as in
    `small', `medium' or `large'.

    \begin{defn}
        An \textit{entity reference} is a collection of attributes that refer
        to a real-world entity which can be formed by following an organizing
        principle (or order) of the information source where the attributes are
        all located.
    \end{defn}

    The organizing principle of an information source is simply an order that
    facilitates processing that information source.
    For example, in a CSV file it would be the rule that up until the first
    comma on each line we should find a color value.
    For operations on databases such as joins, the organizing principles is
    built around data records.
    For named entity recognition, the organizing principle is built around
    mentions.

    A good generalization of the notions in the above examples put in the
    context of entity resolution might be that of a
    \textit{reference}\cite{Ben2009Swoosh}.
    For in this context the rules for processing records, mentions or lines in
    CSV files are used simply to find references to real-world entities.

    The organizing principle of an information source shouldn't be confused with
    the structure of the information in that source.
    A single table record in a database might have a few attribute that
    reference one entity (for example a company or a person) and a few other
    attributes that reference another entity (a building).
    For entity resolution tasks that value geolocation above other things, the 
    fields that reference the building will constitute the entity reference.
    For tasks that identify people, the entity reference will be comprised from
    the attributes that reference a person or a company.
    The organizing principle of the information source by which attributes are
    collected as entity references by the entity resolution task is extrinsic to
    the information source and is somehow linked to the entity resolution task
    itself.
    We say `linked' because the organizing principle may or may not be an
    integral part of the entity resolution task.
    For instance, entity alignment tasks rely on external knowledge bases and
    entity matchers based on deep learning rely on machine learning for
    determining the organizing principle of every data source they consume.

    The other important distinction is that the attributes that make up an
    entity reference are collected from the same information source.
    This creates a clear belonging relationship between entity reference and
    information source.
    
    Let's suppose that we are looking to identify colors across multiple
    information sources.
    In one information source the colors are represented as `red', `green' or 
    `blue'.
    In another information source they are represented as `CC0000', `00CC00' or
    `0000CC'.
    Because we are looking for colors we decide that our organizing principle
    should dictate that references contain only one attribute.
    However, up until now we haven't articulated a way to tell the entity
    resolution task that.
    Namely: how do we tell the entity resolution task that it's looking for
    something that is a \textit{color}.
    Or, formulated as a question: what is \textit{color}?

    \begin{defn}
        A \textit{trait} is a semantic rule that guides the entity resolution
        task in recognizing the organizing principle of each information source
        that it uses as input.
    \end{defn}

    Every entity resolution task has goals it must accomplish.
    Some tasks stitch news articles together, others help organize content
    across social networks or deduplicate the information stored in database
    tables.
    For each task there are rules of interpreting the information at hand that
    stand out above others.
    In our case, we must simply tell our task that it should be interested only
    in colors and how those colors might look like depending on information
    source.
    
    The notion that's the most similar to that of a trait is the `feature':
    \textit{an individual measurable property or characteristic of a
    phenomenon}\cite{bishop2006pattern}.
    The mechanics of how traits and features function is near identical.
    The difference between a trait as defined here and a feature is a matter of
    perspective.
    Features are part of a more objective perspective on information.
    Traits as they are defined here are concerned with the contextual meaning of
    things.
    In the broadest sense, they are nothing more than algorithms that extract
    information according to the subjective goal of the entity resolution task.
    A trait can help the entity resolution task extract one or more attributes
    for a given entity reference that can't be inferred from the underlying
    information alone.
    For example, a trait called `profitable' is different from a physical
    representation in the form of a table column or a JSON property.
    Imagine an entity resolution task that is meant to find companies across
    databases.
    One of the traits this entity resolution task is looking for is the company
    profitability.
    In a database table, it looks at values stored in a column called `profit'.
    In a no-SQL collection, it looks at the values stored under the
    `isProfitable' property.
    Yet, the entity resolution task simply looks for the `profitable' trait in
    both these information sources to help shape the entity references it finds.

    To sum up, an attribute of an entity reference always exists in an
    information source, whereas a trait of the entity resolution task does not.
    Entity resolution tasks extract attributes that they assign to entity
    references based on the traits specific to each task.
    This point of view is supported by the need to design entity resolution
    systems that are extensible based on their sources of
    information\cite{fever2009}\cite{magellan2020}\cite{oyster2012}.

    So far we have not discussed what happens if entity references point to the
    same real-world entity.
    
    \begin{defn}
        We refer to the logical group of entity references that point to
        the same real-world entity according to the entity resolution task's
        parameters as the \textit{entity profile} of that real-world entity.
    \end{defn}

    It is important to understand that the entity profile and the real-world
    entity are not the same.
    The entity profiles are simply manifestations of the input parameters of the
    entity resolution task (namely the information sources and the traits) and
    they represent the output of the entity resolution task.
    
    By using the terms \textit{information source}, \textit{attribute},
    \textit{trait}, \textit{entity reference} and \textit{entity profile} we can
    finally define the entity resolution problem.

    \section{Problem Definition}\label{sec:problem}

    From a process perspective, entity resolution is usually comprised of four
    different activities\cite{Pap19,Tal11}:
    \begin{itemize}
        \item \textit{entity reference extraction} --- the first phase of entity
        resolution deals with extracting entity references from multiple
        sources, as detailed in the introduction of this paper;
        \item \textit{blocking} and \textit{filtering} --- at this stage, the
        entity resolution task groups in the same `block' entities that share
        similar traits and then within each block all entities that can't
        possibly match are filtered \textit{out}; the intent here is to allow
        entity resolution tasks to work with large amounts of data \cite{Pap19};
        \item \textit{matching} --- one of the defining activities of the entity
        resolution process and compares entity references to one another;
        \item \textit{clustering} --- the other defining activity within the
        entity resolution process whereby matching references are collected
        together in entity profiles.
    \end{itemize}

    We are concerned with matching and clustering in this paper.
    However, we cannot formalize that process without a quick overview of entity
    reference extraction because it is within the extraction phase that the
    input to the matching and clustering phases is formed.

    Given:
    \begin{itemize}
        \item $n \in N^*$ pairs (($S_i$, $n_i$), $1 \leq i \leq n$, $n_i \in N$), 
        where $S_i$ is an information source and $n_i$ is the number of messages
        in $S_i$,
        \item $m_i \in N^*$ traits, ($t_{ij}$, $1 \leq j \leq m_i$) for each
        information source $S_i$ so that
        \item each trait $t_{ij}$ can generate at most $a_{ij} \in N^*$
        attributes,
    \end{itemize}
    an entity resolution task $ER$ applies each trait $t_{ij}$ onto the source
    $S_i$, ($1 \leq i \leq n$, $1 \leq j \leq m_i$), in order to extract
    $r_{{S_i}k}$ entity references where
    ($0 \leq k \leq n_i \cdot \sum^{m_i}_{j=1}a_{ij}$).
    We denote with $R$ the domain of all the $r_{{S_i}k}$ entity references that
    were extracted by $ER$.
    Similarly, we denote with $P$ the domain of all entity profiles.
    The matching and clustering step of the $ER$ task is a function
    $E: R \rightarrow P$ that converts individual references to entity profiles.
    The exact definition of this function is specific for each mathematical
    model that underpins entity resolution.
    The data representation of the entity profiles in $P$ also varies according
    to the mathematical model of choice.
    On the other hand, the data representations of the concepts that are part of
    building $R$ do not differ between mathematical models.

    The most basic building block in the context of entity resolution is the
    humble attribute.
    We denote with $A$ the domain of all possible attributes that can be used by
    the task $ER$ described above.

    In this context, let $r_{{S_i}{k_j}}$ denote the $k_j$th ($1 \leq j \leq k$)
    entity reference extracted from the source $S_i$.
    Then $r_{{S_i}{k_j}}$ is always represented as a tuple
    ($a_r$, $r \in N^*$, $a_r \in A$), $r_{{S_i}{k_j}} \in A^r$.
    Note that $r$ varies for each $k_j$.
    The concept of an order external to the information source and the manner in
    which we defined traits serves as the reason why tuples express entity
    references sufficiently well.

    A trait is then a function that helps construct entity references.
    The definition domain of this function materializes only when the $ER$ task
    extracts a given entity reference $r_{{S_i}{k_j}}$.
    The domain of this function is then the domain of all of the attributes in
    the source $S_i$ that could be meaningful in the context of constructing the
    entity reference $r_{{S_i}{k_j}}$ from a certain subjective perspective.
    We denote this domain with $A_{{S_i}{k_j}}$.
    Then the trait is defined as a function $t: S_i \rightarrow A^x$, where
    $x \in N^*$ and $x$ varies with $k_j$ as defined above for entity
    references.
    This formal definition of the trait satisfies the needs of any extraction
    process specific to entity resolution.

    During the extraction process, the entity resolution task applies multiple
    traits with the end result of building the domain of entity references:

    \[
        Ref = \bigcup_{i \in N^*,1 \leq j \leq k} r_{{S_i}{k_j}}
        \textrm{, with r, S and k specified above}
    \]

    The $Ref$ domain is constructed through entity reference extraction and
    after the entities have been extracted, we are no longer concerned with the
    input information sources.
    Another observation is that every type of entity resolution task can be made
    to work on such an input domain\cite{Pap19}.
    A task that uses two or more input sources can be reduced by concatenating
    entity references extracted from those sources.
    So, every entity resolution task can be viewed at its core as either a
    data deduplication or a clustering task.
    Regardless of the optics, the input domain contains only entity references.
    In our view, the entity resolution task is agnostic of the provenance of the
    entity references it processes.
    
    That allows us to formalize the entity resolution task as a function:
    \[
        e : Ref \rightarrow \{P_x \mid x \in \mathbb{N}, P_x \in P \},
    \]
    
    where $P$ is the domain of entity profiles and $x$ is the number of
    resulting entity profiles.
    We denote with $Res=\{P_x \mid x \in \mathbb{N}, P_x \in P\}$ as the domain
    of all entity resolution task results.
    Note that $Res$ \textit{is a set} because the nature of the entity
    resolution task is to cluster or deduplicate, both operations being widely
    understood as operations that result in creating unique items (be they
    clusters or unique entity references).

    It stands to reason that the domain $Res$ contains a result which is the
    closest to our understanding of the real world from a given perspective in
    a given context.

    \begin{defn}
        We define the \textit{ground truth} $G$ of an input entity reference
        domain $Ref$ the entity resolution result comprised of entity profiles
        such that
        $\{P_g \mid g \in N\textrm{, $P_g$ describes a real-world entity
        completely}\}$.
    \end{defn}

    In other words the ground truth is the ideal entity resolution result for 
    given input data, but in a given context, from a given perspective.
    This result contains only those profiles that describe real-world entities.

    Now that we know what the ideal result of an entity resolution task looks
    like, we can at least formulate a measure of the task's relative qualitative
    performance with respect to the ground truth of the task's input.
    
    \begin{defn}
    Given an entity resolution task $e: Ref \rightarrow Res$ and the ground
    truth $G$ for the input entity reference domain $Ref$, a \textit{quality
    metric} is an idempotent function $q: Res \rightarrow \mathbb{R}$ that
    measures the similarity between the input of the function and $G$.
    \end{defn}

    In practice, we should strive to implement quality metric as \textit{pure}
    functions in the sense that running them on a computer will not entail any
    side-effects.

    As stated before, the shape and structure of the items in $Res$, including
    $G$, depends on the mathematical model underpining a specific entity
    resolution task implementation.
    Because the input domain of the quality metrics changes according to the
    used mathematical model, the set of quality metrics available for describing
    the performance of a certain entity resolution task $Q = \{q_y \mid y \in
    \mathbb{N}\}$ is dependent on the mathematical model used for implementing
    that task.

    \subsection[ere]{Entity Resolution Evaluation}\label{sec:ere}

    In the strictest sense, comparing two entity resolution tasks is possible as
    long as they can be evaluated using the same set $Q$ of qualitative
    evaluators.
    Note that if the entity resolution $e_1$ can be evaluated using a set of
    quality metrics $Q_1$ and $e_2$ can be evaluated using a set of quality
    metrics $Q_2$ such that $Q_1 \cap Q_2 \neq \emptyset$, we have a set of
    quality metrics $Q_\cap = Q_1 \cap Q_2$ that allows us to compare $e_1$ and
    $e_2$ based on $Q_\cap$.
    Future developments will seek to further loosen this constraint to allow
    comparing entity resolution tasks that do not share a set of qualitative
    evaluators.

    \subsection[cert]{Comparing Entity Resolution Tasks}\label{subsec:cert}

    \begin{defn}
        Given
        \begin{itemize}
            \item the set of entity references $Ref$,
            \item a set of quality metrics $Q$ that are common to all
            entity resolution tasks $e_i : Ref -> Res$, $i \in \mathbb{N}$
            \item the ground truth $G$ for the input domain $Ref$,
            \item the quality metric results
            $V_i=\{q_{i} \mid q_{i} = q(G, e_i), q \in Q\}, i \in \mathbb{N}$,
        \end{itemize}   
        we define the comparison of the entity resolution tasks above as
        $C : (
            \{V_i \mid i \in \mathbb{N}\}
            \times
            \{V_j \mid j \in \mathbb{N}\},
            i \neq j
        ) \rightarrow \{-1, 0, 1\}$ such that:
        \[ 
C(V_i, V_j) = \left\{
\begin{array}{ll}
      -1,~\textrm{if $e_i$ is less fit for purpose than $e_j$}\\
      0,~\textrm{if $e_i$ is as fit for purpose as $e_j$}\\
      1,~\textrm{if $e_i$ is better fit for purpose than $e_j$}\\
\end{array} 
\right. 
\]

    \end{defn}

    \section[mm]{Mathematical Models}\label{sec:mm}

    As we have mentioned, mathematical models play a big part in how entity
    resolution functions represent output data.
    That means that the ground truth used for evaluating the performance of an
    entity resolution task is also represented in a specific way depending on 
    the mathematical model used by the entity resolution task under evaluation.
    
    The mathematical models available for entity resolution range from ones
    based on complex networks and graph theory\cite{Li2020} to ones based on
    probabilities\cite{fs1969} or algebra\cite{Tal11,Ben2009Swoosh}.

    We have already pointed out that the output domain of the entity resolution
    task when viewed as a function is shaped by the underlying mathematical
    model.
    In this paper we describe three of these models and how these models
    influence the data structures used by the entity resolution tasks that
    implement them.

    \subsection[fsm]{Fellegi-Sunter Model}\label{subsec:fsm}

    In the late 1960s Ivan Fellegi and Alan Sunter wrote the seminal
    paper\cite{fs1969} for what they called record linkage and what would later
    become known as entity resolution.
    To this day, their mathematical model based on probability theory is the
    most popular way of formalizing the entity resolution problem.
    In this mathematical model, entity resolution is a function that aids in 
    probabilistic decision making.
    
    \subsubsection[fsm-desc]{Description}\label{subsec:fsm-desc}

    The original model defines record linkage as an operation over two input
    sets, $A$ and $B$.
    The result of the operation is a set containing pairs of items
    $X = \{(a, b), a \in A, b \in B\}$ or $X = A \times B$.
    
    The model then deconstructs $X$ to two disjoint subsets $X = M \cup U$: $M$ for pairs that
    contain matching items and $U$ for pairs that contain non-matching items.

    \begin{align}
        M &= \{(a, b) | a == b, a \in A, b \in B\}~\textrm{and} \\
        U &= \{(a, b) | a \neq b, a \in A, b \in B\}
    \end{align}

    The model proposes that if $a \in A$ and $b \in B$ are two vectors, their
    comparison will also be a vector denoted $\gamma \in \varGamma$, where
    $\varGamma$ denotes the set of all possible values of $\gamma$.
    The comparison between $a$ and $b$ is performed between each corresponding
    element in each of the two vectors.
    The elements of $\gamma$ vary according to the type of comparison that is
    performed\cite{winkler1990}.

    In this context, a match decision function can make one of three decisions
    regarding a pair from $X$ according to the F-S model:
    
    \begin{itemize}
        \item consider it a \textit{link ($A_1$)} between the items;
        \item consider it a \textit{non-link ($A_3$)} between the items, and
        \item leave things undecided, thus marking it as a \textit{possible link
              ($A_2$)}.
    \end{itemize}
    
    Then a linkage rule is a function $L:\varGamma \rightarrow D$,
    $D=\{d(\gamma)\}$, where
    
    \begin{align}
        &d(\gamma) = \{P(A_1|\gamma),P(A_2|\gamma),P(A_3|\gamma)\};
        &\textrm{so that}~\sum_{i=1}^{3}P(A_i|\gamma) = 1\nonumber.
        \gamma \in \varGamma\nonumber
    \end{align}

    In other words, for a $\gamma \in \varGamma$, $L(\gamma) = \{P(A_1|\gamma),
    P(A_2|\gamma), P(A_3|\gamma)\}$ so that the probabilities sum up to $1$.

    When using this model the purpose of entity resolution is to find the
    optimal linkage rule.
    The F-S theorem defines it as the linkage rule that minimizes
    $P(A_2|\gamma)$.
    In other words, the optimal linkage rule according to the F-S model is the
    linkage rule without uncertainties.
    This optimization inclination of the model makes it very well suited for
    both rule-based\cite{oyster2012} and machine learning\cite{deepm2020}
    implementations.
    
    In this universe, two conditional probabilities become interesting:

    \begin{align}
        m(\gamma)&=P(\gamma(a, b) | (a, b) \in M)~\textrm{and}\nonumber\\
        u(\gamma)&=P(\gamma(a, b) | (a, b) \in U)\textrm{.}\nonumber
    \end{align}

    \noindent
    $m(\gamma)$ is the probability of a $\gamma$ comparison vector given an
    $(a, b)$ \textit{link}.
    $u(\gamma)$ is the probability of a $\gamma$ comparison vector given an
    $(a, b)$ \textit{non-link}.
    For brevity, we have not used the complete notation pertaining to records
    from the original paper.

    The two probabilities outlined above are important because they help us
    express the probabilities of the Type I and Type II statistical errors
    associated with the decision function that sits at the core of the linkage
    rule definition above.
    The probabilities of the two types of error are expressed as:

    \begin{align}
        \mu&=\sum_{\gamma \in \varGamma}u(\gamma)P(A_1|\gamma)\textrm{,~and}\nonumber\\
        \lambda&=\sum_{\gamma \in \varGamma}m(\gamma)P(A_3|\gamma)\nonumber
    \end{align}

    where:

    \begin{itemize}
        \item $\mu$ represents the Type I error of items that were erroneusly
        linked (i.e pairs in $M$ that do not belong in $M$), and
        \item $\lambda$ represents the Type II error of items that were
        erroneously \textit{not} linked (i.e. pairs in $U$ that do not belong in
        $U$).
    \end{itemize}

    \subsubsection[fms-term]{How Does It Relate}\label{fsm-term}

    To link the Fellegi-Sunter model to our own terminology, let's start by
    talking about $a \in A$ and $b \in B$.
    The original F-S model calls $A$ and $B$ ``populations'' and $a$ and $b$,
    ``population elements''.
    A distinction is made between records, denoted with $\alpha(a)$ and
    $\beta(b)$, and population elements.
    Most of the original paper refers to records.

    It is safe for the reader to assume that in our terminology the population
    elements correspond to real-world entities and the records correspond to
    entity references.
    However, our view on $\alpha(a)$ and $\beta(b)$ is more broad in that we
    think of them not merely as records, but as functions that associate a
    representation as records(entity references) to the population elements
    (information about real-world entities).
    We used the notion of traits or extraction traits to express this concept.
        
    The $A$ and $B$ populations equate roughly to our notion of information
    sources.
    The ``population'' concept is a particularization of our information source
    concept because it is defined as a set in the original paper whereas our
    definition includes other possible data representations.
    The concept is also more general than our own concept because we defined
    information sources under the assumption of machines being able to process
    the information within them, whereas a population is a more general concept
    to which this restriction need not apply.

    The Fellegi-Sunter model works under the assumption that there are two
    populations $A$ and $B$ and defines the linkage rule as a function that uses
    conditional probabilities that involve both populations.
    On the other hand, our $Ref$ domain for entity references is clearly
    solitary.
    Using our terminology we would denote $A$ with $S_1$ and $B$ with $S_2$.
    We start constructing our $Ref$ domain by using traits to extract $n_1$
    entity references from $S_1$ and $n_2$ entity references from $S_2$.
    Then $Ref = \{r_{{S_i}{k_i}},~1 \leq k_i \leq n_i, i \in {1, 2} \}$.
    \textcolor{green}{both $S$ and $k$ have the same index $i$? If there are two sources ($S_1$ nd $S_2$), then $i$ will be 1 or 2. The $k$ index is used for indexing the references, and I would denote by $1 \leq k \leq n_i$.}
    \textcolor{orange}{let's say there's 5 elements in $S_1$ and 10 elements in
    $S_2$. $k_1$ goes from 1 to 5 and $k_2$ goes from 1 to 10}
    The result domain is expressed as $X = A \times B$ in this model.
    It is subsequently split into $M$ and $U$ the sets of matching and
    non-matching pairs in $X$, respectively.
    The $X$ in the original model is equivalent to $Res$ in our terminology.
    $Res$ can be defined as:
    \begin{align}
        &Res = \{(r_{{S_i}{k}}, r_{{S_j}{l}}),~1 \leq k \leq |S_i|,~1
        \leq l \leq |S_j|, i \neq j, i,j \in \{1, 2\}\}\textrm{,}\nonumber
    \end{align}
    where $|S_i|$ denotes the number of entity references extracted from the
    source $S_i$.
    Our concept of an entity profile translates to a pair of entity references,
    with $M$ being the set containing the entity profiles that refer to the same
    real-world entity and $U$ being the set of entity profiles that don't refer
    to the same real-world entity.

    Recall that the shape of the ground truth and that of the output of the
    entity resolution tasks that this model use is determined by the definition
    of the output domain of the entity resolution function.
    Concretely, we expect entity resolution tasks that implement this model to
    output a set of pairs of tuples.
    For example, if $r_{11}=(1, 2, 3)$ and $r_{21}=(4, 5, 6)$ are two entity
    references that refer to the same real-world entity, this model would return
    the pair $((1, 2, 3), (4, 5, 6))$.

    A natural question is whether, instead of a single set of pairs of tuples we
    should expect two sets of pairs of tuples that would correspond to the two
    subsets $M$ and $U$ from the original paper.
    For $M$ this doesn't seem to be a problem because in practice $M$'s growth
    would be correlated to the maximum number of entity references extracted
    from a single information source.
    On the other hand, constructing $U$ seems to be prohibitively expensive.
    Let's imagine that we're grouping the extracted entity references in vectors
    by their information source of provenance.
    Then $U$ will grow proportionally to the length of the vector product of
    these vectors.
    So, for practical reasons, we are of the opinion that the output of the
    entity resolution function as well as the ground truth should contain only
    the items that should be in $M$.

    With the above definition for $Res$ we also observe that this model always
    compares two entity references and can not explain linking more than two
    entities\cite{Tal11}.
    While there is work that extended the model to compare more than two 
    entity references\cite{Kon19}, we will not touch upon that in this paper.
    
    As a consequence, we can't represent transitive links between entity
    references with this model.
    This means that if the entity reference $r_1$ matches $r_2$ and
    $r_2$ matches $r_3$, we can't infer that $r_1$ matches $r_3$ by using this
    model\cite{Tal11}.

    \subsubsection[fms-measure]{Measurements}\label{fsm-measure}

    The quality metrics that are specific to this mathematical model can be any
    of the very broadly used statistical metrics.
    Because we have a clear definition of the Type I and Type II errors, we can
    give a clear context for the notions of true and false positive and true and
    false negative.
    In defining them we make use of the $M$ and $U$ sets as they are defined in
    the original paper describing the F-S model.

    Depending on where a pair output by the entity resolution function is
    expected to be found, we define:

    \begin{itemize}
        \item \textbf{true positives} as pairs predicted to be in $M$ that
        should be in $M$,
        \item \textbf{false positives}, or type I errors, as pairs predicted to
        be in $M$, but should be in $U$,
        \item \textbf{true negatives} as pairs predicted to be in $U$ that
        should in $U$, and
        \item \textbf{false negatives}, or type II errors, as pairs predicted to
        be in $U$, but should be in $M$.
    \end{itemize}

    Quality metrics for this model work by comparing a ground truth that
    containing the set of ideal pairs of tuples to the entity resolution task's
    output.
    $M$ is effectively the ground truth in this context.
    Because using $U$ is prohibitive in practice, we will rely only on concepts
    that have to do with $M$: true and false positives, as well as false
    negatives.

    With this in mind we finally define the three quality metrics that are
    widely used with this model:

    \begin{align}
    Precision&=\frac{TP}{TP+FP} \\
    Recall&=\frac{TP}{TP+FN} \\
    F_1 Score&=2 \cdot \frac{Precision \cdot Recall}{Precision+Recall}
    \end{align}

    \textit{Precision} (or the positivive predictive value) is defined as the
    number of correct predictions that were made in relation to the total number
    of predictions that were made.
    \textit{Recall} (or sensitivity) is defined as the number of correct
    predictions that were made in relation to the total number of positive
    predictions that could have been made (which corresponds to the number of
    items in the ground truth).
    The \textit{$F_1$} score is the harmonic mean of the precision and the
    recall and it is used to capture the tradeoff between precision and
    recall\cite{hitesh2012}.

    \subsection[algebraic]{Algebraic Model}\label{subsec:algebraic}

    Initially proposed as a model to discern information quality in entity
    resolution over large datasets\cite{tal2007algebraic}, the algebraic model
    for entity resolution was later refined to describe entity resolution
    itself\cite{Tal11}.
    Although entity resolution systems and tasks have not been implemented with
    this specific model in mind, its elegance allows it to be applied to
    virtually any entity resolution task.
    This model relaxes the constraint of the Fellegi-Sunter model\hyperref{subsec:fsm}
    around item uniqueness within an information source.
    It also addresses transitive matches.
    
    \subsubsection[algdesc]{Description}\label{subsubsec:algdesc}

    The core tenet of this model is that entity resolution is an \textit{
    equivalence relation} which is a concept defined in set theory.
    Let's start by defining a few algebraic terms.
    
    \begin{defn} If $X$ is a set and $R$ is a rule that takes two elements from
    $X$, $x$ and $y$, and tells us whether $x$ is in relationship $y$ in the way
    denoted with $R$ then we call $R$ a \textit{binary relation} on
    $X$\cite{hoffman1971linear}.
    \end{defn}

    We can represent $R$ as a function $R:X \times X \rightarrow \{0,1\}$ where
    $R(x,y)=1$ if $x$ is in relation $R$ with $y$ and $R(x,y)=0$ otherwise.
    
    \begin{defn}If R is a binary relation on the set $X$, it is convenient to
    write $xRy$ when $R(x, y) = 1$. A binary relation R is called:

    \begin{enumerate}
        \item reflexive, if $xRx, \forall x \in X$;
        \item symmetric, if $yRx \implies xRy, \forall x,y \in X$;
        \item transitive, if $xRz \land yRz \implies xRz, \forall x,y,z \in X$;
    \end{enumerate}

    An \textit{equivalence relation} on $X$ is a reflexive, symmetric, and
    transitive binary relation on $X$\cite{hoffman1971linear}.
    \end{defn}

    Equivalence relations already seem to describe a lot of the matching process
    that happens in entity resolution.
    Let's observe that equivalence relations work on sets for now and move on.

    An equivalence relation over a set $X$ generates an equivalence class.

    \begin{defn}Suppose $R$ is an equivalence relation on the set X. If $x$ is
    an element of X, let $\bar{x}$ denote the set $\{y \in X | xRy\}$.
    $\bar{x}$ is called the \textit{equivalence class} of $x$ (for the
    equivalence relation R)\cite{hoffman1971linear}.
    \end{defn}

    Since R is an equivalence relation, the equivalence classes for R have the
    following properties\cite{hoffman1971linear}:
    \begin{enumerate}
        \item $xRx \in \bar{x} \implies \bar{x} \neq \emptyset$,
        \item $y\in\bar{x}\iff~x\in\bar{y},~\forall x,y \in X$ because
        equivalence relations are symmetric,
        \item $\forall x,y \in X$ $\bar{x}$ is either identical with $\bar{y}$
        or they have no elements in common\cite{hoffman1971linear,Tal11}.
    \end{enumerate}
    
    The last fundamental notion from set theory that is needed to construct the
    algebraic model of entity resolution is the \textit{partition} of a set.

    \begin{defn}
        A \textit{partition} of a set $X$ is a disjoint collection of non-empty
        subsets of $X$ whose union is $X$\cite{halmos1960naive,Tal11}.
    \end{defn}

    Things come together starting with the simple observation that an element
    belonging to one of the sets from a partition over $X$ is in a relation with
    itself and other elements from the same set.
    We say that this relation is \textit{induced} by the partition.
    From the point of view of the above relation, we say that it
    \textit{generated} the partition.

    When the partition in question is the set of equivalence classes over $X$,
    then the the induced relation is an equivalence relation.
    Conversely, an equivalence relation $R$ in $X$ will generate a partition
    over $X$ comprised by the equivalence classes of $X$\cite{halmos1960naive,
    Tal11}.

    In the algebraic model, entity resolution is stated to be an equivalence
    relation over the set comprising all input entity references.
    Let's denote this relation with $ER$ and state that if $xERy$ then x and y
    are linked\cite{Tal11}.

    It is important to note the difference between the equivalence relation and
    pairwise matching.
    Matching itself is still done pairwise, however it does not really matter
    how.
    On the other hand, the equivalence relation induces a partition over the
    initial set of references by definition.
    Also by definition, it is transitive.
    If we denote by $P$ the partition generated by the entity resolution
    relation and we have $xERy \land yERz \implies xERz$ and that means that 
    at least $\{x, y, z\} \in P$.
    We usually call the members of the partition $P$ \textit{clusters} and we
    say that $x$, $y$ and $z$ belong to the same cluster or are clustered
    together.

    A last point of notice is that entity resolution tasks do not necessarily
    generate a unique outcome\cite{Tal11}.
    Depending on the order in which entity references are processed, entity
    resolution tasks might generate different partitions.
    The entity resolution tasks where the order in which entity references are
    processed is irrelevant are called \textit{sequence neutral}\cite{Tal11}.

    \subsubsection[algrel]{How Does It Relate}\label{subsubsec:algrel}

    The algebraic model is very similar to our own view over entity resolution.
    For one, the notions of attribute and entity reference essentially mean the
    same thing.
    Then the idea that entity resolution is an equivalence relation over an
    input set of entity references matches our view on the matter.
    The difference lies only with the specific requirement that the input be
    a set in the algebraic model.
    This means that we won't be able to apply the algebraic model on input data
    that is not a set.
    
    Having to use only distinct references in the input is a problem in practice
    because most real data where we care about entity resolution is ambiguous.
    We can overcome this limitation by introducing attributes that serve as
    disambiguation clues in the matching process.

    We should note that the algebraic model does not concern itself with how
    entity references end up being part of the same set (i.e it does not relay
    too much information about the extraction process).
    Therefore the notions of information source and trait have a marginal impact
    here.
    In fact, the only place where we feel traits might be important is in the
    discussion related to the order in which entity references are processed.
    In our view, the order of processing is one of the traits of the entity
    resolution task.

    With respect to the output, it's plain that both the ground truth and the
    result of the entity resolution task itself are partitions over the input
    set of entity references.
    The ground truth $G$ is the ideal partition over the input set of entity
    references $Ref$ with regard to the relationship that is defined by entity
    references that reference the same real-world entity.
    Each cluster in the ground truth refers to exactly one real-world entity.
    The result of the entity resolution task $Res$ is another partition over
    $Ref$.

    An interesting discussion here is around designing algorithms that construct
    partitions generated by the entity resolution relation.
    In the introduction of this paper we discussed \textit{clustering} as one of
    the central tasks in entity resolution.
    Constructing partitions is such a task.
    For example, using a particular \textit{merge} function the
    \textit{Swoosh}\cite{Ben2009Swoosh} family of algorithms (to which we will
    come back in the next section) seems to be a good candidate for building
    partitions.
    
    Another interesting aspect is that by abstracting away matching, we can
    easily apply this model for any type of entity resolution algorithm that
    implements either deterministic (i.e exact) matching or
    \hyperref[subsec:fsm]{probabilistic matching}.

    \subsubsection[algeval]{Quality Metrics}\label{subsubsec:algeval}

    The quality metrics for entity resolution under the algebraic model have to
    do with computing the similarity of partitions.
    In our case, these metrics are applied to compare the partitions $G$ and
    $Res$.
    Many such metrics exist\cite{hitesh2012} and we chose to cover some of them
    which either by lineage or by their popularity have peaked our interest.

    The first among these is the Rand index introduced by William Rand in
    1971\cite{rand1971}.
    This index computes the similarity of partitions by comparing pairs of
    elements formed from the elements of the clusters of each partition.
    Supposing that:
    \begin{itemize}
        \item $a$ is the number of pairs that are in the same cluster in $G$ and
        also the same cluster in $Res$,
        \item $b$ is the number of pairs that are in the same cluster in $G$ but
        are not in the same cluster in $Res$,
        \item $c$ is the number of pairs that are in the same cluster in $Res$
        but are not in the same cluster in $G$,
        \item $d$ is the number of pairs that are in different clusters.
    \end{itemize}
    then the Rand index is defined by the quantity
    $\frac{a+d}{a+b+c+d}$\cite{adjrand2001}.

    The minimum value of the Rand index is 0, meaning nothing matches, while its
    maximum value is 1, indicating a perfect match.
    The obvious use of the Rand index is to rate the similarity of two
    partitions.
    Its less obvious use is to consider $1 - Rand Index$ the distance between
    two partitions.

    The expected value of the Rand index of two random partitions does not take
    a constant value (say zero)\cite{adjrand2001}.
    That lead to the introduction of the adjusted Rand index which follows the
    same principles, but uses a specific distribution for
    randomness\cite{adjrand1985}.
    Without going into detail, we can provide a formula for the adjusted Rand
    index by using the quantities we defined for the Rand index\cite{Tal11}.
    \[
        Adjusted~Rand~Index = \frac{
            a - (\frac{(a+b)\dot(a+c)}{a+b+c+d})
        }{
            \frac{(2\dot a+b+c)}{2}-(\frac{(a+b)\dot(a+c)}{a+b+c+d})
        }
    \]

    The third quality metric we can use for comparing partition similarity is
    the Talburt-Wang index.
    The merit of this metric is that it closely follows the Rand index, but is
    much simpler to compute\cite{Tal11}.
    \[
        TWI = \frac{\sqrt{|G|\dot|Ref|}}{|G \cap Ref|},
    \]
    where if $X$ is a set then $|X|$ represents the size of $X$.

    The Talburt-Wang index takes values between 0 and 1 as well, with 1
    indicating a perfect match.

    Other similarity metrics used to compare partitions are the pairwise and
    cluster precision and recall metrics.
    

    \subsection[serf]{Stanford Entity Resolution Framework Model}\label{subsec:serf}

    \textcolor{green}{Please mention the model's parents and add a reference!}
    \begin{itemize}
        \item views entity resolution as the result of applying a match function, a merge function and a domination relationship on top of a set
        \item allows evaluating transitive matches
        \item the match function can be anything as long as we can act with a yes/no action on its result
        \item the merge function should be distributive (merge(a, merge(b, c)) = merge(merge(a, b), c))
        \item provides a very flexible way of modeling entity resolution (the comparison and addition of integer numbers is a valid basis for an entity resolution task on the set of integer numbers)
        \item the entity profile is either one of the original input items or an information that dominates all the information that was used to create it
    \end{itemize}

    \section{Experiments}

    We have designed an experiment to showcase the insights that are provided by
    each of the mathematical models presented thus far.

    \subsection{Data}

    The data for the experiments presented in this paper is available on
    GitHub\cite{expdata2023}.
    It is comprised of a subset of ten items from the `Buy' dataset which is a
    part of the Abt-Buy dataset\cite{vldb2010}.
    Most of these items were chosen to be near-duplicates.
    
    We chose tabular data because of the support this type of data structure has
    received in the programming community\cite{pandas2010,pandas2023}.
    From now on it should be assumed that the information sources are tables.
    The tables are stored on disk in CSV files.
    Each CSV file has a first row containing the column headings and subsequent
    rows containing records within which the fields are separated by comma.
    
    Since we want full control over the entity resolution process, we choose to
    generate the data sources from our initial dataset.
    We split the Buy table into two tables having the following columns:
    \begin{enumerate}[label=\textbullet~Table~\arabic*:,leftmargin=*]
        \item `name',`manufacturer' and `price'
        \item `description',`name'.
    \end{enumerate}
    These two tables will be the input information sources for our experiments.

    Our entity reference extraction traits assume that all data is string data.
    We chose strings because numbers or dates are very easy to disambiguate and
    in real-world scenarios we very rarely deal with data that is not formatted
    as strings.
    The traits built into our extraction pipeline are mapped one-to-one to table
    columns.
    For each record, the each trait extracts a single attribute.
    The ordering of the data enforced by the traits is left-to-right,
    top-to-bottom reading order.
    This setup results in all entity references from Table 1 being tuples with
    three values, while entity references from Table 2 will have two values.

    For example, let's use the following record from our initial `Buy' table:
    \begin{center}
        \begin{tabular}[b]{|l|l|l|l|l|}
            \hline
            id&name&description&manufacturer&price\\
            \hline
            205554724&Seiko SXDA04& & &\$138.00\\
            \hline
        \end{tabular}
    \end{center}

    Our data generation strategy would split it into two records, each placed
    in the corresponding table.
    The first record would be:
    
    \begin{center}
        \begin{tabular}[b]{|l|l|l|l|}
            \hline
            name&manufacturer&price \\
            \hline
            Seiko SXDA04& &\$138.00 \\
            \hline
        \end{tabular}
    \end{center}

    The second record would look like this:

    \begin{center}
        \begin{tabular}[b]{|l|l|l|}
            \hline
            description&name \\
            \hline
            &Seiko SXDA04 \\
            \hline
        \end{tabular}
    \end{center}

    Then the traits we have built for this experiment will extract the followin
    two entity references:
    \begin{itemize}
        \item \texttt{(`Seiko SXDA04', ` ',`\$138.00')},
        \item \texttt{(` ',`Seiko SXDA04')}.
    \end{itemize}

    The entity resolution task receives its input as a list containing two lists
    each of which contain 10 entity references.

    Because we are in full control of the input sources for the entity
    resolution task, we can be very precise about how the ground truth should
    look like.
    We posit that each record in our initial `Buy' table refers to a single
    real-world entity.
    We also say that there are no two records in the initial `Buy' table that
    refer to the same real-world entity.
    Consequently, the two records that we generate from each record in the
    initial table refer to the same real-world entity.
    This gives us a very easy way to generate the ground truth in whatever
    format each mathematical model expects.

    In our experiments we use the \texttt{ppjoin}\cite{ppjoin} matching
    algorithm because of its simplicity.
    PPJoin stands for Position Prefix Join and it is an algorithm designed to
    find similarities between records based on the Jaccard
    index\cite{jaccard1912,finley1996}.
    Intuitively, a lower threshold will match a larger number of items because
    the prefix being used for matching shortens.
    A higher threshold should match fewer items, but with less chance for error.
    In our experiments the Jaccard threshold will be denoted with $t$.
    The mathematical model that fits this algorithm best is the Fellegi-Sunter
    probabilistic model because it works using records and attributes.
    The Jaccard threshold manipulates the probabilities whether the attributes
    match or not.
    Even so, we shall learn that we can use the other mathematical models to
    better understand this algorithm's performance.

    As to how we will use the algorithm: we will take 100 measurements of each
    qualitative evaluator for each mathematical model for incremental values of
    the Jaccard similarity threshold $t$ within the $[0\ldots0.99]$ closed
    interval in discrete steps of $0.01$.

    \subsection{Fellegi-Sunter Model Results}
    
    With the F-S model both the ground truth and the results of the entity
    resolution algorithm are represented as a list of pairs of entity
    references.
    We generate the ground truth for the F-S model by iterating over both
    input data sources using the same cursor and outputting pairs of records.
    We notice duplicate CSV records that only differ on the `id' column.

    Intuition tells us that for values of the Jaccard similarity threshold ($t$)
    that are either too low or too high we should have lower precision.
    For higher thresholds we should also have lower recall values, whereas for
    lower values the recall should be higher.
    This intuition is captured best in \hyperref[fsfig]{the figure} that
    captures \texttt{ppjoin}'s results at various values of the Jaccard
    threshold $t$.

    \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{fsfig}
        \caption{Fellegi-Sunter Results}\label{fsfig}
    \end{figure}

    When the Jaccard similarity threshold is set high ($t \geq 0.66$), we end up
    with few matches, but those matches are true.
    As we lower the threshold ($0.27 \leq t < 0.66$), recall increases and
    precision does not drop.
    However, when we lower the threshold too much ($t < 0.27$) we start losing
    precision even though we get better and better recall.
    
    These results are exactly what we expect.
    So what can we learn from here?

    We know that we reach high levels of recall only when we start losing
    precision.
    Though, we should notice that precision and recall are lost in steps, not
    continually.
    If we were tuning the algorithm for a specific use case one thing that would
    definitely be troubling us would be big gap in precision around the $0.27$
    mark.
    We have no insight into why that happens and we lose more than half of our
    precision.
    One suspicion we could have is that we don't know \textit{how much} each
    entity reference matches another and that might be the cause of the gap.
    This shortcoming is inherent to this model of entity matching because the
    model itself minimizes ambiguity.
    
    Another aspect has to do with duplicates.
    We get worse and worse precision as we lower the Jaccard threshold.
    Again, there's no transparent reason why this happens.
    We can speculate that if we have two entity references from our original
    dataset that are near-duplicates, a low Jaccard threshold might cause
    cross-references of those entities to be predicted as matches.
    For example, if we have two entity references that have different `id'
    values while the other attributes are extremely similar, our data generation
    algorithm will generate four records that are near identical.
    Then \texttt{ppjoin} will flag matches between all four records, instead of
    matching them pairwise.
    On a small, controlled dataset like this it's easy to speculate however on a
    bigger, real-world data set we would probably like better insight.

    Lastly, we undestand that intuitively by lengthening the Jaccard threshold
    we are increasing precision and lessening recall, however there is again no
    insight into why we lose so much of our recall around the $t=0.5$ mark.
    Could we at least get an order of magnitude for our lack of sensitivity?

    \subsection{Algebraic Model}

    \textit{TODO}

    \subsection{Stanford Entity Resolution Framework Model}

    \textit{TODO}

    \section[conclusion]{Conclusions}\label{section:conclusions}

    The work speaks about how we can use mathematical models to evaluate entity resolution results. It also provides easy rules of thumb to identify the model that's best suited to interpret the results of an ER task.
    
    \section[future]{Future Work}\label{section:future}
    \begin{itemize}
        \item result translation between math models
        \item ground truth and data generation for ER tasks
    \end{itemize}


    \bibliographystyle{plain} % We choose the "plain" reference style
    \bibliography{er-general,er-related-work,er-additional-references,er-software}
\end{document}
