%! Author = andrei.olar@ubbcluj.ro
%! Date = 03.09.2023

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{cite}
\usepackage{array}

% Document
\begin{document}
    \theoremstyle{definition}
    \newtheorem{defn}{Definition}[section]
    
    \section{Abstract}\label{sec:abstract}


    \section{Introduction}\label{sec:introduction}
    Entity resolution is the task of finding out whether two pieces of
    information refer to the same real-world item or not.
    There's more restrictive definitions of entity resolution as the task of
    identifying and linking representations of data from two or more
    sources\cite{Qia17}.
    We share the opinion that identifying and linking data consitutes a more
    specialized process\cite{Tal11}.
    The task of gathering information about a generic pound of potatoes across
    various markets is still an entity resolution task, in our view.
    
    Probably because of its generic nature, entity resolution also goes by many
    different names such as: record linkage, data deduplication, merge-purge,
    named entity recognition, entity alignment or entity
    matching\cite{Tal11,fever2009}.

    Entity resolution has many practical applications ranging from linking
    medical records to doing background checks on persons of interest to 
    identifying plagiarism.
    As an aside, combining information from different media (sound, images,
    motion, smell, etc) is also a form of entity resolution and opens up many
    avenues into its future as an area of study.

    The glue that binds all of the above examples has at least the following
    ingredients:
    \begin{itemize}
        \item\textit{a concern for a real-world entity}, for if the task were
        not about a something from our world, the resolution would not have a
        clear goal;
        \item\textit{no implication as to the used method}, because if we
        stipulated a certain way to perform entity resolution, we might not be
        able to;
        \item\textit{the representation of information is computer friendly},
        because if it weren't, it might not be possible to study the task by
        means of computer science.
    \end{itemize}

    We must clarify regarding the concern for a real-world entity that entity
    resolution tasks do not deal with real-world entities directly but with
    information about them\cite{Tal11}.
    In other words, entity resolution tasks typically don't know about the
    real-world objects or entities they are meant to resolve\cite{Chen09}.
    Naturally, entity resolution tasks will not perform perfectly from a
    qualitative standpoint.
    We are interested to find common ground for measuring entity resolution
    performance.
    
    Historically, entity resolution performance was measured in many ways, some
    of which (true positive rate, false positive rate) have been contested for
    certain use cases\cite{Goga2015}.
    Our work means to standardize qualitative entity resolution quality
    measurement across the board.
    We base our methodology on the mathematical models that underpin entity
    resolution tasks.

    The first sections of this paper go over the contributions it makes and the
    work related to this paper.
    Afterwards we start our journey towards the main subject by introducing some
    basic entity resolution terminology which is used throughout the paper.
    Following that, we describe the scientific problem of comparing the
    performance of entity resolution tasks.
    Next we go through three classical mathematical models and show how each of
    them influences entity resolution evaluation.
    After the conceptual framework is ready, we provide experimental data to see
    how the concepts map to practice.
    In the closing sections, we provide some insights that we are now able to 
    glean from the results, draw some conclusions and finally hint towards
    future work on this topic.

    \section{Contributions}\label{sec:contributions}

    This paper makes the following contributions:

    \begin{itemize}
        \item materializes the connection between mathematical model and entity
        resolution task
        \item provides a generic framework for measuring the qualitative
        performance of entity resolution tasks
        \item introduces a software system that uses this framework
    \end{itemize}

    \section{Related Work}\label{sec:related}
    
    Because of the generality of the topic, there is much existing work that
    relates in some way or another to this paper.
    However, three categories of papers share the most concerns with this one:
    entity resolution system presentations, syntheses of theoretical models for
    entity resolution and syntheses of evaluation metrics for entity resolution.

    The papers that introduce entity resolution systems relate to this one
    through the shared goal of generalizing how entity resolution tasks are run
    and, therefore, evaluated.
    Most of the systems referenced here provide at least a few generic metrics
    to evaluate entity resolution performance.
    They 

    At the dawn of the 21st century, FEBRL\cite{febrl2002} emerged as one of the
    first extensible systems to tackle measuring entity resolution performance.
    It was succeeded by FEVER\cite{fever2009} and OYSTER\cite{oyster2012}. 
    These systems have all paved the way to generalizing our understanding about
    entity resolution and to ever more improvements in the quality of entity
    resolution tasks.
    Later on, systems like `Papers with Code'\cite{papwithcode2019} brought in
    a social dimension to how we compare the qualitative performance of entity
    resolution tasks.
    Newer systems resemble frameworks more than they resemble
    applications\cite{magellan2020,jedai2017}.
    These systems are open-source and encourage social collaboration around
    entity resolution, too.

    
    There exist numerous syntheses on the theoretical models for entity
    resolution\cite{fs1969,Ben2009Swoosh,Tal11}.
    Some of the available models have been compared and
    explained\cite{Tal11,tal2013}.

    Ways in which entity resolution tasks are similar even though they seem
    to have nothing in common have been discussed in papers that deal with
    certain steps of the entity resolution process\cite{Pap19,Chen09}.

    \section{Terminology}\label{sec:terminology}

    Given that we have just reserved the term ``entity'' for real-world items,
    we have to come up with new terms for the concepts used in the context of
    entity resolution.
    We start by saying that entity resolution operates on information sources.

    \begin{defn}
        An \textit{information source} represents a sequence of decoded messages
        that originate somewhere and can be processed by the entity resolution
        task which reads them over a communication channel.
    \end{defn}

    The terms `decoded', `message' and `communication channel' refer to concepts
    from the field of information theory\cite{ash2012it}.
    
    An entity resolution task can operate on one or multiple information
    sources.
    Information sources may be bounded (such as files or databases) or unbounded
    (such as streams).
    
    In the context of the above definition, the entity resolution task is the
    computer program doing the processing.
    What sets entity resolution programs apart is the notions they use to
    process information from a particular source.

    \begin{defn}
        An \textit{attribute} is information about an entity that has a certain
        meaning in a context given by a frame of reference and certain rules of
        interpretation.
    \end{defn}

    Attributes by themselves are not enough to describe an entity.
    For example, `red' fully describes an entity only if we are looking for
    colors.
    If we're looking for fruit, `red' is only one of several attributes that
    help describe fruit.
    For example we might be interested in `sour' or `sweet' as well as in
    `small', `medium' or `large'.

    \begin{defn}
        An \textit{entity reference} is a collection of attributes that refer
        to a real-world entity which can be formed by following an organizing
        principle (or order) of the information source where the attributes are
        all located.
    \end{defn}

    The organizing principle of an information source is simply an order that
    facilitates processing that information source.
    For example, in a CSV file it would be the rule that up until the first
    comma on each line we should find a color value.
    For operations on databases such as joins, the organizing principles is
    built around data records.
    For named entity recognition, the organizing principle is built around
    mentions.

    A good generalization of the notions in the above examples put in the
    context of entity resolution might be that of a
    \textit{reference}\cite{Ben2009Swoosh}.
    For in this context the rules for processing records, mentions or lines in
    CSV files are used simply to find references to real-world entities.

    The organizing principle of an information source shouldn't be confused with
    the structure of the information in that source.
    A single table record in a database might have a few attribute that
    reference one entity (for example a company or a person) and a few other
    attributes that reference another entity (a building).
    For entity resolution tasks that value geolocation above other things, the 
    fields that reference the building will constitute the entity reference.
    For tasks that identify people, the entity reference will be comprised from
    the attributes that reference a person or a company.
    The organizing principle of the information source by which attributes are
    collected as entity references by the entity resolution task is extrinsic to
    the information source and is somehow linked to the entity resolution task
    itself.
    We say `linked' because the organizing principle may or may not be an
    integral part of the entity resolution task.
    For instance, entity alignment tasks rely on external knowledge bases and
    entity matchers based on deep learning rely on machine learning for
    determining the organizing principle of every data source they consume.

    The other important distinction is that the attributes that make up an
    entity reference are collected from the same information source.
    This creates a clear belonging relationship between entity reference and
    information source.
    
    Let's suppose that we are looking to identify colors across multiple
    information sources.
    In one information source the colors are represented as `red', `green' or 
    `blue'.
    In another information source they are represented as `CC0000', `00CC00' or
    `0000CC'.
    Because we are looking for colors we decide that our organizing principle
    should dictate that references contain only one attribute.
    However, up until now we haven't articulated a way to tell the entity
    resolution task that.
    Namely: how do we tell the entity resolution task that it's looking for
    something that is a \textit{color}.
    Or, formulated as a question: what is \textit{color}?

    \begin{defn}
        A \textit{trait} is a semantic rule that guides the entity resolution
        task in recognizing the organizing principle of each information source
        that it uses as input.
    \end{defn}

    Every entity resolution task has goals it must accomplish.
    Some tasks stitch news articles together, others help organize content
    across social networks or deduplicate the information stored in database
    tables.
    For each task there are rules of interpreting the information at hand that
    stand out above others.
    In our case, we must simply tell our task that it should be interested only
    in colors and how those colors might look like depending on information
    source.
    
    The notion that's the most similar to that of a trait is the `feature':
    \textit{an individual measurable property or characteristic of a
    phenomenon}\cite{bishop2006pattern}.
    The mechanics of how traits and features function is near identical.
    The difference between a trait as defined here and a feature is a matter of
    perspective.
    Features are part of a more objective perspective on information.
    Traits as they are defined here are concerned with the contextual meaning of
    things.
    In the broadest sense, they are nothing more than algorithms that extract
    information according to the subjective goal of the entity resolution task.
    A trait can help the entity resolution task extract one or more attributes
    for a given entity reference that can't be inferred from the underlying
    information alone.
    For example, a trait called `profitable' is different from a physical
    representation in the form of a table column or a JSON property.
    Imagine an entity resolution task that is meant to find companies across
    databases.
    One of the traits this entity resolution task is looking for is the company
    profitability.
    In a database table, it looks at values stored in a column called `profit'.
    In a no-SQL collection, it looks at the values stored under the
    `isProfitable' property.
    Yet, the entity resolution task simply looks for the `profitable' trait in
    both these information sources to help shape the entity references it finds.

    To sum up, an attribute of an entity reference always exists in an
    information source, whereas a trait of the entity resolution task does not.
    Entity resolution tasks extract attributes that they assign to entity
    references based on the traits specific to each task.
    This point of view is supported by the need to design entity resolution
    systems that are extensible based on their sources of
    information\cite{fever2009}\cite{magellan2020}\cite{oyster2012}.

    So far we have not discussed what happens if entity references point to the
    same real-world entity.
    
    \begin{defn}
        We refer to the logical group of entity references that point to
        the same real-world entity according to the entity resolution task's
        parameters as the \textit{entity profile} of that real-world entity.
    \end{defn}

    It is important to understand that the entity profile and the real-world
    entity are not the same.
    The entity profiles are simply manifestations of the input parameters of the
    entity resolution task (namely the information sources and the traits) and
    they represent the output of the entity resolution task.
    
    By using the terms \textit{information source}, \textit{attribute},
    \textit{trait}, \textit{entity reference} and \textit{entity profile} we can
    finally define the entity resolution problem.

    \section{Problem Definition}\label{sec:problem}

    From a process perspective, entity resolution is usually comprised of four
    different activities\cite{Pap19,Tal11}:
    \begin{itemize}
        \item \textit{entity reference extraction} --- the first phase of entity
        resolution deals with extracting entity references from multiple
        sources, as detailed in the introduction of this paper;
        \item \textit{blocking} and \textit{filtering} --- at this stage, the
        entity resolution task groups in the same `block' entities that share
        similar traits and then within each block all entities that can't
        possibly match are filtered \textit{out}; the intent here is to allow
        entity resolution tasks to work with large amounts of data \cite{Pap19};
        \item \textit{matching} --- one of the defining activities of the entity
        resolution process and compares entity references to one another;
        \item \textit{clustering} --- the other defining activity within the
        entity resolution process whereby matching references are collected
        together in entity profiles.
    \end{itemize}

    We are concerned with matching and clustering in this paper.
    However, we cannot formalize that process without a quick overview of entity
    reference extraction because it is within the extraction phase that the
    input to the matching and clustering phases is formed.

    Given:
    \begin{itemize}
        \item $n \in N^*$ pairs (($S_i$, $n_i$), $1 \leq i \leq n$, $n_i \in N$), 
        where $S_i$ is an information source and $n_i$ is the number of messages
        in $S_i$,
        \item $m_i \in N^*$ traits, ($t_{ij}$, $1 \leq j \leq m_i$) for each
        information source $S_i$ so that
        \item each trait $t_{ij}$ can generate at most $a_{ij} \in N^*$
        attributes,
    \end{itemize}
    an entity resolution task $ER$ applies each trait $t_{ij}$ onto the source
    $S_i$, ($1 \leq i \leq n$, $1 \leq j \leq m_i$), in order to extract
    $r_{{S_i}k}$ entity references where
    ($0 \leq k \leq n_i \cdot \sum^{m_i}_{j=1}a_{ij}$).
    We denote with $R$ the domain of all the $r_{{S_i}k}$ entity references that
    were extracted by $ER$.
    Similarly, we denote with $P$ the domain of all entity profiles.
    The matching and clustering step of the $ER$ task is a function
    $E: R \rightarrow P$ that converts individual references to entity profiles.
    The exact definition of this function is specific for each mathematical
    model that underpins entity resolution.
    The data representation of the entity profiles in $P$ also varies according
    to the mathematical model of choice.
    On the other hand, the data representations of the concepts that are part of
    building $R$ do not differ between mathematical models.

    The most basic building block in the context of entity resolution is the
    humble attribute.
    We denote with $A$ the domain of all possible attributes that can be used by
    the task $ER$ described above.

    In this context, let $r_{{S_i}{k_j}}$ denote the $k_j$th ($1 \leq j \leq k$)
    entity reference extracted from the source $S_i$.
    Then $r_{{S_i}{k_j}}$ is always represented as a tuple
    ($a_r$, $r \in N^*$, $a_r \in A$), $r_{{S_i}{k_j}} \in A^r$.
    Note that $r$ varies for each $k_j$.
    The concept of an order external to the information source and the manner in
    which we defined traits serves as the reason why tuples express entity
    references sufficiently well.

    A trait is then a function that helps construct entity references.
    The definition domain of this function materializes only when the $ER$ task
    extracts a given entity reference $r_{{S_i}{k_j}}$.
    The domain of this function is then the domain of all of the attributes in
    the source $S_i$ that could be meaningful in the context of constructing the
    entity reference $r_{{S_i}{k_j}}$ from a certain subjective perspective.
    We denote this domain with $A_{{S_i}{k_j}}$.
    Then the trait is defined as a function $t: S_i \rightarrow A^x$, where
    $x \in N^*$ and $x$ varies with $k_j$ as defined above for entity
    references.
    This formal definition of the trait satisfies the needs of any extraction
    process specific to entity resolution.

    During the extraction process, the entity resolution task applies multiple
    traits with the end result of building the domain of entity references:

    \[
        Ref = \bigcup_{i \in N^*,1 \leq j \leq k} r_{{S_i}{k_j}}
        \textrm{, with r, S and k specified above}
    \]

    The $Ref$ domain is constructed through entity reference extraction and
    after the entities have been extracted, we are no longer concerned with the
    input information sources.
    Another observation is that every type of entity resolution task can be made
    to work on such an input domain\cite{Pap19}.
    A task that uses two or more input sources can be reduced by concatenating
    entity references extracted from those sources.
    So, every entity resolution task can be viewed at its core as either a
    data deduplication or a clustering task.
    Regardless of the optics, the input domain contains only entity references.
    In our view, the entity resolution task is agnostic of the provenance of the
    entity references it processes.
    
    That allows us to formalize the entity resolution task as a function:
    \[
        e : Ref \rightarrow \{P_x \mid x \in \mathbb{N}, P_x \in P \},
    \]
    
    where $P$ is the domain of entity profiles and $x$ is the number of
    resulting entity profiles.
    We denote with $Res=\{P_x \mid x \in \mathbb{N}, P_x \in P\}$ as the domain
    of all entity resolution task results.
    Note that $Res$ \textit{is a set} because the nature of the entity
    resolution task is to cluster or deduplicate, both operations being widely
    understood as operations that result in creating unique items (be they
    clusters or unique entity references).

    It stands to reason that the domain $Res$ contains a result which is the
    closest to our understanding of the real world from a given perspective in
    a given context.

    \begin{defn}
        We define the \textit{ground truth} $G$ of an input entity reference
        domain $Ref$ the entity resolution result comprised of entity profiles
        such that
        $\{P_g \mid g \in N\textrm{, $P_g$ describes a real-world entity
        completely}\}$.
    \end{defn}

    In other words the ground truth is the ideal entity resolution result for 
    given input data, but in a given context, from a given perspective.
    This result contains only those profiles that describe real-world entities.

    Now that we know what the ideal result of an entity resolution task looks
    like, we can at least formulate a measure of the task's relative qualitative
    performance with respect to the ground truth of the task's input.
    
    \begin{defn}
    Given an entity resolution task $e: Ref \rightarrow Res$ and the ground
    truth $G$ for the input entity reference domain $Ref$, a \textit{qualitative
    evaluator} is an idempotent function $q: Res \rightarrow \mathbb{R}$ that
    measures the similarity between the input of the function and $G$.
    \end{defn}

    In practice, we should strive to implement qualitative evaluators as
    \textit{pure} functions in the sense that running them on a computer will
    not entail any side-effects.

    As stated before, the shape and structure of the items in $Res$, including
    $G$, depends on the mathematical model underpining a specific entity
    resolution task implementation.
    Because the input domain of the qualitative evaluators changes according to
    the used mathematical model, the set of qualitative evaluators available for
    describing the qualitative performance of a certain entity resolution task
    $Q = \{q_y \mid y \in \mathbb{N}\}$ is dependent on the mathematical model
    used for implementing that task.

    \subsection[ere]{Entity Resolution Evaluation}\label{sec:ere}

    In the strictest sense, comparing two entity resolution tasks is possible as
    long as they can be evaluated using the same set $Q$ of qualitative
    evaluators.
    Note that if the entity resolution $e_1$ can be evaluated using a set of
    qualitative evaluators $Q_1$ and $e_2$ can be evaluated using a set of
    qualitative evaluators $Q_2$ such that $Q_1 \cap Q_2 \neq \emptyset$, we
    have a set of qualitative evaluators $Q_\cap = Q_1 \cap Q_2$ that allows us
    to compare $e_1$ and $e_2$ based on $Q_\cap$.
    Future developments will seek to further loosen this constraint to allow
    comparing entity resolution tasks that do not share a set of qualitative
    evaluators.

    \subsection[cert]{Comparing Entity Resolution Tasks}\label{subsec:cert}

    \begin{defn}
        Given
        \begin{itemize}
            \item the set of entity references $Ref$,
            \item a set of qualitative evaluators $Q$ that are common to all
            entity resolution tasks $e_i : Ref -> Res$, $i \in \mathbb{N}$
            \item the ground truth $G$ for the input domain $Ref$,
            \item the qualitative evaluator results $V_i=\{q_{i} \mid q_{i} = q(G, e_i), q \in Q\},
            i \in \mathbb{N}$,
        \end{itemize}   
        we define the comparison of the entity resolution tasks above as
        $C : (
            \{V_i \mid i \in \mathbb{N}\}
            \times
            \{V_j \mid j \in \mathbb{N}\},
            i \neq j
        ) \rightarrow \{-1, 0, 1\}$ such that:
        \[ 
C(V_i, V_j) = \left\{
\begin{array}{ll}
      -1,~\textrm{if $e_i$ is less fit for purpose than $e_j$}\\
      0,~\textrm{if $e_i$ is as fit for purpose as $e_j$}\\
      1,~\textrm{if $e_i$ is better fit for purpose than $e_j$}\\
\end{array} 
\right. 
\]

    \end{defn}

    \section[mm]{Mathematical Models}\label{sec:mm}

    As we have mentioned, mathematical models play a big part in how entity
    resolution functions represent output data.
    That means that the ground truth used for evaluating the performance of an
    entity resolution task is also represented in a specific way depending on 
    the mathematical model used by the entity resolution task under evaluation.
    
    The mathematical models available for entity resolution range from ones
    based on complex networks and graph theory\cite{Li2020} to ones based on
    probabilities\cite{fs1969} or algebra\cite{Tal11,Ben2009Swoosh}.

    We have already pointed out that the output domain of the entity resolution
    task when viewed as a function is shaped by the underlying mathematical
    model.
    In this paper we describe three of these models and how these models
    influence the data structures used by the entity resolution tasks that
    implement them.

    \subsection[fsm]{Fellegi-Sunter Model}\label{subsec:fsm}

    In the late 1960s Ivan Fellegi and Alan Sunter wrote the seminal
    paper\cite{fs1969} for what they called record linkage and what would later
    become known as entity resolution.
    To this day, their mathematical model based on probability theory is the
    most popular way of formalizing the entity resolution problem.
    In this mathematical model, entity resolution is a function that aids in 
    probabilistic decision making.
    
    \subsubsection[fsm-desc]{Description}\label{subsec:fsm-desc}

    The original model defines record linkage as an operation over two input
    sets, $A$ and $B$.
    The result of the operation is a set containing pairs of items
    $X = \{(a, b), a \in A, b \in B\}$ or $X = A \times B$.
    
    The model then deconstructs $X$ to two disjoint subsets $X = M \cup U$: $M$ for pairs that
    contain matching items and $U$ for pairs that contain non-matching items.

    \begin{align}
        M &= \{(a, b) | a == b, a \in A, b \in B\}~\textrm{and} \\
        U &= \{(a, b) | a \neq b, a \in A, b \in B\}
    \end{align}

    The model proposes that if $a \in A$ and $b \in B$ are two vectors, their
    comparison will also be a vector denoted $\gamma \in \varGamma$, where
    $\varGamma$ denotes the set of all possible values of $\gamma$.
    The comparison between $a$ and $b$ is performed between each corresponding
    element in each of the two vectors.
    The elements of $\gamma$ vary according to the type of comparison that is
    performed\cite{winkler1990}.

    In this context, a match decision function can make one of three decisions
    regarding a pair from $X$ according to the F-S model:
    
    \begin{itemize}
        \item consider it a \textit{link ($A_1$)} between the items;
        \item consider it a \textit{non-link ($A_3$)} between the items, and
        \item leave things undecided, thus marking it as a \textit{possible link
              ($A_2$)}.
    \end{itemize}
    
    Then a linkage rule is a function $L:\varGamma \rightarrow D$,
    $D=\{d(\gamma)\}$, where

    
    \begin{align}
        &d(\gamma) = \{P(A_1|\gamma),P(A_2)|\gamma,P(A_3)|\gamma\};
        \gamma \in \varGamma\nonumber\nonumber\\
        &\textrm{so that}~\sum_{i=1}^{3}P(A_i|\gamma) = 1\nonumber.
    \end{align}

    \textcolor{green}{oare e $P(A_2)|\gamma$ sau $P(A_2|\gamma)$? unde se inchide paranteza?}

    \textcolor{green}{oare se poate preciza cum arata functia $L$? s-a zis ca e o functie, s-a precizat domeniul (multimea vectorilor rezultati din compararea a 2 items a si b), codomeniul $D$ (functia $d(\gamma)$), dar nu se zice nimic de functia efectiva}

    When using this model the purpose of entity resolution is to find the
    optimal linkage rule.
    The F-S theorem defines it as the linkage rule that minimizes
    $P(A_2|\gamma)$.
    In other words, the optimal linkage rule according to the F-S model is the
    linkage rule without uncertainties.
    This optimization inclination of the model makes it very well suited for
    both rule-based\cite{oyster2012} and machine learning\cite{deepm2020}
    implementations.
    
    In this universe, two conditional probabilities become interesting:

    \begin{align}
        m(\gamma)&=P(\gamma(a, b) | (a, b) \in M)~\textrm{and}\nonumber\\
        u(\gamma)&=P(\gamma(a, b) | (a, b) \in U)\textrm{.}\nonumber
    \end{align}

    \noindent
    $m(\gamma)$ is the probability of a $\gamma$ comparison vector given an
    $(a, b)$ \textit{link}.
    $u(\gamma)$ is the probability of a $\gamma$ comparison vector given an
    $(a, b)$ \textit{non-link}.
    For brevity, we have not used the complete notation pertaining to records
    from the original paper.

    The two probabilities outlined above are important because they help us
    express the probabilities of the Type I and Type II statistical errors
    associated with the decision function that sits at the core of the linkage
    rule definition above.
    The probabilities of the two types of error are expressed as:

    \begin{align}
        \mu&=\sum_{\gamma \in \varGamma}u(\gamma)P(A_1|\gamma)\textrm{,~and}\nonumber\\
        \lambda&=\sum_{\gamma \in \varGamma}m(\gamma)P(A_3|\gamma)\nonumber
    \end{align}

    where:

    \begin{itemize}
        \item $\mu$ represents the Type I error of items that were erroneusly
        linked (i.e pairs in $M$ that do not belong in $M$), and
        \item $\lambda$ represents the Type II error of items that were
        erroneously \textit{not} linked (i.e. pairs in $U$ that do not belong in
        $U$).
    \end{itemize}

    \subsubsection[fms-term]{How Does It Relate}\label{fsm-term}

    To link the Fellegi-Sunter model to our own terminology, let's start by
    talking about $a \in A$ and $b \in B$.
    The original F-S model calls $A$ and $B$ ``populations'' and $a$ and $b$,
    ``population elements''.
    A distinction is made between records, denoted with $\alpha(a)$ and
    $\beta(b)$, and population elements.
    Most of the original paper refers to records.

    It is safe for the reader to assume that in our terminology the population
    elements correspond to real-world entities and the records correspond to
    entity references.
    However, our view on $\alpha(a)$ and $\beta(b)$ is more broad in that we
    think of them not merely as records, but as functions that transform 
    \textcolor{green}{I would say "that represent / that associate a representation to"} 
    the
    population elements (information about real-world entities) into records
    (entity references).
    We used the notion of traits or extraction traits to express this concept.
        
    The $A$ and $B$ populations equate roughly to our notion of information
    sources.
    The ``population'' concept is a particularization of our information source
    concept because it is defined as a set in the original paper whereas our
    definition includes other possible data representations.
    The concept is also more general than our own concept because we defined
    information sources under the assumption of machines being able to process
    the information within them, whereas a population is a more general concept
    to which this restriction need not apply.

    The Fellegi-Sunter model works under the assumption that there are two
    populations $A$ and $B$ and defines the linkage rule as a function that uses
    conditional probabilities that involve both populations.
    On the other hand, our $Ref$ domain for entity references is clearly
    solitary.
    Using our terminology we would denote $A$ with $S_1$ and $B$ with $S_2$.
    We start constructing our $Ref$ domain by using traits to extract $n_1$
    entity references from $S_1$ and $n_2$ entity references from $S_2$.
    Then $Ref = \{r_{{S_i}{k_i}},~1 \leq k_i \leq n_i, i \in {1, 2} \}$.
    \textcolor{green}{both $S$ and $k$ have the same index $i$? If there are two sources ($S_1$ nd $S_2$), then $i$ will be 1 or 2. The $k$ index is used for indexing the references, and I would denote by $1 \leq k \leq n_i$.}

    The result domain is expressed as $X = A \times B$ in this model.
    It is subsequently split into $M$ and $U$ the sets of matching and
    non-matching pairs in $X$, respectively.
    The $X$ in the original model is equivalent to $Res$ in our terminology.
    $Res$ can be defined as:
    \begin{align}
        &Res = \{(r_{{S_i}{k}}, r_{{S_j}{l}}),~1 \leq k \leq |S_i|,~1
        \leq l \leq |S_j|, i \neq j, i,j \in \{1, 2\}\}\textrm{,}\nonumber
    \end{align}
    where $|S_i|$ denotes the number of entity references extracted from the
    source $S_i$.
    Our concept of an entity profile translates to a pair of entity references,
    with $M$ being the set containing the entity profiles that refer to the same
    real-world entity and $U$ being the set of entity profiles that don't refer
    to the same real-world entity.

    Recall that the shape of the ground truth and that of the output of the
    entity resolution tasks that this model use is determined by the definition
    of the output domain of the entity resolution function.
    Concretely, we expect entity resolution tasks that implement this model to
    output a set of pairs of tuples.
    \textcolor{green}{I would give an example for this tuple}

    A natural question is whether, instead of a single set of pairs of tuples we
    should expect two sets of pairs of tuples that would correspond to the two
    subsets $M$ and $U$ from the original paper.
    For $M$ this doesn't seem to be a problem because in practice $M$'s growth
    would be correlated to the maximum number of entity references extracted
    from a single information source.
    On the other hand, constructing $U$ seems to be prohibitively expensive.
    Let's imagine that we're grouping the extracted entity references in vectors
    by their information source of provenance.
    Then $U$ will grow proportionally to the length of the vector product of
    these vectors.
    So, for practical reasons, we are of the opinion that the output of the
    entity resolution function as well as the ground truth should contain only
    the items that should be in $M$.

    With the above definition for $Res$ we also observe that this model always
    compares two entity references and can not explain linking more than two
    entities\cite{Tal11}.
    While there is work that extended the model to compare more than two 
    entity references\cite{Kon19}, we will not touch upon that in this paper.
    
    As a consequence, we can't represent transitive links between entity
    references with this model.
    This means that if the entity reference $r_1$ matches $r_2$ and
    $r_2$ matches $r_3$, we can't infer that $r_1$ matches $r_3$ by using this
    model\cite{Tal11}.

    \subsubsection[fms-measure]{Measurements}\label{fsm-measure}

    The qualitative evaluators specific to this mathematical model are among
    the best known.
    Because we have a clear definition of the Type I and Type II errors, we can
    give a clear context for the notions of true and false positive and true and
    false negative.
    In defining them we make use of the $M$ and $U$ sets as they are defined in
    the original paper describing the F-S model.

    Depending on where the pair output by the entity resolution function is
    expected to be, we define:

    \begin{itemize}
        \item \textbf{true positives} as pairs in $M$ that should be in $M$,
        \textcolor{green}{I would introduce the "prediction" notion; for instance, \textbf{true positives} as pairs predicted to be in $M$ and that should be in $M$,}
        \item \textbf{false positives}, or type I errors, as pairs that should
        be in $U$, but are in $M$,
        \textcolor{green}{\textbf{false positives}, or type I errors, as pairs that should be in $U$, but they are predicted to be in $M$,}
        \item \textbf{true negatives} as pairs in $U$ that belong in $U$, 
        \textcolor{green}{see above!} 
        and
        \item \textbf{false negatives}, or type II errors, as pairs that should
        be in $M$, but are in $U$. 
        \textcolor{green}{see above!} 
    \end{itemize}

    Qualitative evaluators for this model work by comparing a ground truth that
    containing the set of ideal pairs of tuples to the entity resolution task's
    output.
    $M$ is effectively the ground truth in this context.
    Because using $U$ is prohibitive in practice, we will rely only on concepts
    that have to do with $M$: true and false positives, as well as false
    negatives.

    With this in mind we finally define the three qualitative evaluators that
    are widely used with this model:

    \begin{align}
    Precision&=\frac{TP}{TP+FP} \\
    Recall&=\frac{TP}{TP+FN} \\
    F_1 Score&=2 \cdot \frac{Precision \cdot Recall}{Precision+Recall}
    \end{align}

    \subsection[serf]{Stanford Entity Resolution Framework Model}\label{subsec:serf}

    \textcolor{green}{Please mention the model's parents and add a reference!}
    \begin{itemize}
        \item views entity resolution as the result of applying a match function, a merge function and a domination relationship on top of a set
        \item allows evaluating transitive matches
        \item the match function can be anything as long as we can act with a yes/no action on its result
        \item the merge function should be distributive (merge(a, merge(b, c)) = merge(merge(a, b), c))
        \item provides a very flexible way of modeling entity resolution (the comparison and addition of integer numbers is a valid basis for an entity resolution task on the set of integer numbers)
        \item the entity profile is either one of the original input items or an information that dominates all the information that was used to create it
    \end{itemize}

    \subsection[algebraic]{Algebraic Model}\label{subsec:algebraic}

    John R. Talburt proposes this model. 
    \textcolor{green}{add a reference!}
    \begin{itemize}
        \item ER is an equivalence relation on a set of items
        \item reflexive, symmetric, transitive
        \item ER is the same as the partition generated by a given equivalence relation over an input set
        \item very easy to work with and very comprehensive (transitivity, identity resolution, etc)
        \item the entity profile is a partition class (a cluster)
    \end{itemize}

    \section{Experiments}

    In order to highlight the differences between what each mathematical model
    can provide us with, we have designed an experiment.
    In our experiment we will run an entity resolution task using all three
    mathematical models presented thus far.

    \subsection{Methodology}

    Because we are interested especially in qualitative differences in results
    induced by the mathematical model being used and because these differences
    are visible even on small dataset, we will not monitor questions of scale
    in this experiment.
    Since we are not concerned with the actual score obtained by a certain
    algorithm either, for simplicity we will use the first 10 items from the
    `Buy' table which is a part of the Abt-Buy dataset\cite{vldb2010}.
    
    We split the Buy table into two tables having the following columns:
    \begin{itemize}
        \item the first table will contain the data in the `name', `id',
        `manufacturer' and `price' columns from the original table;
        \item the second table will contain the data in the `description', `id'
        and `name' columns from the original table.
    \end{itemize}
    These two tables will be the input information sources for our experiments.

    We have designed the entity reference extraction such that it causes minimal
    tampering with the qualitative performance readings for the entity
    resolution tasks.
    The information sources are stored on disk as CSV files.
    Each CSV file has a first row containing the column headings and subsequent
    rows containing records within which the fields are separated by comma.
    The traits built into our extraction pipeline are mapped one-to-one to the
    names of the columns in the CSV file.
    For each record, the traits extract a single attribute for each column.
    The ordering of the data enforced by the traits is left-to-right,
    top-to-bottom order.
    This setup results in all entity references being tuples that each have
    three attributes.

    For example, let's say we have the following dataset with a single record.
    \begin{center}
        \begin{tabular}[b]{|l|l|l|l|l|}
            \hline
            id&name&description&manufacturer&price\\
            \hline
            10333848&Linksys EtherFast&24x10/100Base-TX LAN&LINKSYS&64.99 \\
            \hline
        \end{tabular}
    \end{center}

    Our data generation strategy will then create the following two datasets.
    The first dataset will contain the `name', `id', `manufacturer' and `price'
    columns.
    
    \begin{center}
        \begin{tabular}[b]{|l|l|l|l|}
            \hline
            name&id&manufacturer&price \\
            \hline
            Linksys EtherFast&10333848&LINKSYS&64.99 \\
            \hline
        \end{tabular}
    \end{center}

    The second dataset will contain the `description', `id' and `name' columns.

    \begin{center}
        \begin{tabular}[b]{|l|l|l|}
            \hline
            description&id&name \\
            \hline
            24x10/100Base-TX LAN&10333848&Linksys EtherFast \\
            \hline
        \end{tabular}
    \end{center}

    The two generated datasets become the two information sources for the entity
    resolution task.
    The traits we have built for this experiment will extract exactly one entity
    reference from each of the two information sources thus generated:
    \begin{itemize}
        \item (\texttt{Linksys EtherFast,10333848,LINKSYS,64.99}),
        \item (\texttt{24x10/100Base-TX LAN,10333848,Linksys EtherFast}).
    \end{itemize}

    The entity resolution task receives as input all of the entity references
    provided by the extraction phase.
    It can also make use of the information about how data is ordered because
    the traits used for extraction are available to it.

    As a last observation, because we are in full control of the input sources
    of data we can generate the ground truth, too.
    Now let us see what is influenced by the choice of mathematical model.

    \subsection{Fellegi-Sunter Model Results}
    
    With the F-S model both the ground truth and the results of the entity
    resolution algorithm are represented as a set of pairs.
    We generate the ground truth for the F-S model by iterating over both
    input data sources using the same cursor and outputting pairs of records.
    We notice duplicate CSV records that only differ on the `id' column.
    These surely refer to the same real-world entity, however the F-S model is
    unable to capture this information.

    We run an entity matching algorithm of choice and evaluate its performance
    by using the qualitative evaluators specific for the F-S model on the
    results versus the ground truth.
    
    We choose to use the \texttt{ppjoin}\cite{ppjoin} matching algorithm on the
    input datasets, but any probabilistic matching algorithm will do.
    PPJoin matches entities based on prefix strings and in this paper we use it
    only in conjunction with Jaccard similarity.
    Intuition tells us that for values of the Jaccard similarity threshold ($t$)
    that are either too low or too high we should have lower precision.
    For higher thresholds we should also have lower recall values, whereas for
    lower values the recall should be higher.

    \begin{table}[htpb]\label{FS results}
        \caption{FS results}
        \begin{center}
            \begin{tabular}[b]{|cccc|}
                \hline
                t & precision & recall & f1 score \\
                \hline
                0.99 & 1.0 & 0.0009 & 0.0018 \\
                0.75 & 1.0 & 0.2179 & 0.3578 \\
                0.4 & 0.9595 & 1.0 & 0.9793 \\
                0.2 & 0.6920 & 1.0 & 0.8179 \\
                \hline
            \end{tabular}
        \end{center}
    \end{table}

    When the Jaccard similarity threshold is set high ($t > 0.75$), we end up
    with very few matches, but those matches are true.
    As we lower the threshold ($0.4 < t \leq 0.75$), recall increases and
    precision does not drop.
    However, when we lower the threshold too much ($t \leq 0.4$) we start losing
    precision.
    
    All of these results are expected.
    So what can we learn from here?

    For starters, we reach high levels of recall only when we start losing
    precision.
    If we were tuning the Jaccard threshold the granularity for doing so would
    be extremely coarse.
    The recall for $0.4 < t \leq 0.75$ is always $0.3578$ and we have no insight
    as to why.
    This all leads us to understand that we don't know how much each entity
    reference matches another.
    This shortcoming is inherent in the probabilistic model of entity matching
    because in this model something is either a link, a non-link or could be a
    link, but we minimize the probability of ever finding that third option.
    
    Another aspect has to do with duplicates.
    We never reach perfect precision even though it really seems that we should.
    The reason behind that is that \texttt{ppjoin} sometimes finds matches it
    shouldn't. 
    For example, if we have two entity references that only differs on the `id'
    attribute while everything else is identical, \texttt{ppjoin} will flag the
    two references as pointing to the same real-world entity.
    However because the F-S model allows only matching, we have to assume that
    these are in fact different real-world entities in the ground truth.
    Describing this situation more accurately requires a more comprehensive
    model.

    \subsection{Algebraic Model}

    \textit{TODO}

    \subsection{Stanford Entity Resolution Framework Model}

    \textit{TODO}

    \section[conclusion]{Conclusions}\label{section:conclusions}

    The work speaks about how we can use mathematical models to evaluate entity resolution results. It also provides easy rules of thumb to identify the model that's best suited to interpret the results of an ER task.
    
    \section[future]{Future Work}\label{section:future}
    \begin{itemize}
        \item result translation between math models
        \item ground truth and data generation for ER tasks
    \end{itemize}


    \bibliographystyle{plain} % We choose the "plain" reference style
    \bibliography{er-general,er-related-work,er-additional-references}
\end{document}
