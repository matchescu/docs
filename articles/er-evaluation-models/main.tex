%! Author = andrei.olar@ubbcluj.ro
%! Date = 03.09.2023

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{cite}
\usepackage{amsthm}

% Document
\begin{document}
    \theoremstyle{definition}
    \newtheorem{defn}{Definition}[section]
    
    \section{Abstract}\label{sec:abstract}


    \section{Introduction}\label{sec:introduction}
    Entity resolution is the task of finding out whether two pieces of
    information refer to the same real-world item or not.
    There's more restrictive definitions of entity resolution as the task of
    identifying and linking representations of data from two or more
    sources\cite{Qia17}.
    We share the opinion that identifying and linking data consitutes a more
    specialized process\cite{Tal11}.
    The task of gathering information about a generic pound of potatoes across
    various markets is still an entity resolution task, in our view.
    
    Probably because of its generic nature, entity resolution also goes by many
    different names such as: record linkage, data deduplication, merge-purge,
    named entity recognition, entity alignment or entity
    matching\cite{Tal11,fever2009}.

    Entity resolution has many practical applications ranging from linking
    medical records to doing background checks on persons of interest to 
    identifying plagiarism.
    As an aside, combining information from different media (sound, images,
    motion, smell, etc) is also a form of entity resolution and opens up many
    avenues into its future as an area of study.

    The glue that binds all of the above examples has at least the following
    ingredients:
    \begin{itemize}
        \item\textit{a concern for a real-world entity}, for if the task were
        not about a something from our world, the resolution would not have a
        clear goal;
        \item\textit{no implication as to the used method}, because if we
        stipulated a certain way to perform entity resolution, we might not be
        able to;
        \item\textit{the representation of information is computer friendly},
        because if it weren't, it might not be possible to study the task by
        means of computer science.
    \end{itemize}

    We must clarify regarding the concern for a real-world entity that entity
    resolution tasks do not deal with real-world entities directly but with
    information about them\cite{Tal11}.
    In other words, entity resolution tasks typically don't know about the
    real-world objects or entities they are meant to resolve\cite{Chen09}.
    Naturally, entity resolution tasks will not perform perfectly from a
    qualitative standpoint.
    We are interested to find common ground for measuring entity resolution
    performance.
    
    Historically, entity resolution performance was measured in many ways, some
    of which (true positive rate, false positive rate) have been contested for
    certain use cases\cite{Goga2015}.
    Our work means to standardize qualitative entity resolution quality
    measurement across the board.
    We base our methodology on the mathematical models that underpin entity
    resolution tasks.

    The first sections of this paper go over the contributions it makes and the
    work related to this paper.
    Afterwards we start our journey towards the main subject by introducing some
    basic entity resolution terminology which is used throughout the paper.
    Following that, we describe the scientific problem of comparing the
    performance of entity resolution tasks.
    Next we go through three classical mathematical models and show how each of
    them influences entity resolution evaluation.
    After the conceptual framework is ready, we provide experimental data to see
    how the concepts map to practice.
    In the closing sections, we provide some insights that we are now able to 
    glean from the results, draw some conclusions and finally hint towards
    future work on this topic.

    \section{Contributions}\label{sec:contributions}

    This paper makes the following contributions:

    \begin{itemize}
        \item materializes the connection between mathematical model and entity
        resolution task
        \item provides a generic framework for measuring the qualitative
        performance of entity resolution tasks
        \item introduces a software system that uses this framework
    \end{itemize}

    \section{Related Work}\label{sec:related}
    
    Because of the generality of the topic, there is much existing work that
    relates in some way or another to this paper.
    However, three categories of papers share the most concerns with this one:
    entity resolution system presentations, syntheses of theoretical models for
    entity resolution and syntheses of evaluation metrics for entity resolution.

    The papers that introduce entity resolution systems relate to this one
    through the shared goal of generalizing how entity resolution tasks are run
    and, therefore, evaluated.
    Most of the systems referenced here provide at least a few generic metrics
    to evaluate entity resolution performance.
    They 

    At the dawn of the 21st century, FEBRL\cite{febrl2002} emerged as one of the
    first extensible systems to tackle measuring entity resolution performance.
    It was succeeded by FEVER\cite{fever2009} and OYSTER\cite{oyster2012}. 
    These systems have all paved the way to generalizing our understanding about
    entity resolution and to ever more improvements in the quality of entity
    resolution tasks.
    Later on, systems like `Papers with Code'\cite{papwithcode2019} brought in
    a social dimension to how we compare the qualitative performance of entity
    resolution tasks.
    Newer systems resemble frameworks more than they resemble
    applications\cite{magellan2020,jedai2017}.
    These systems are open-source and encourage social collaboration around
    entity resolution, too.

    
    There exist numerous syntheses on the theoretical models for entity
    resolution\cite{fs1969,Ben2009Swoosh,Tal11}.
    Some of the available models have been compared and
    explained\cite{Tal11,tal2013}.

    Ways in which entity resolution tasks are similar even though they seem
    to have nothing in common have been discussed in papers that deal with
    certain steps of the entity resolution process\cite{Pap19,Chen09}.

    \section{Terminology}\label{sec:terminology}

    Given that we have just reserved the term ``entity'' for real-world items,
    we have to come up with new terms for the concepts used in the context of
    entity resolution.
    We start by saying that entity resolution operates on information sources.

    \begin{defn}
        An \textit{information source} represents a sequence of decoded messages
        that originate somewhere and can be processed by the entity resolution
        task which reads them over a communication channel.
    \end{defn}

    The terms `decoded', `message' and `communication channel' refer to concepts
    from the field of information theory\cite{ash2012it}.
    
    An entity resolution task can operate on one or multiple information
    sources.
    Information sources may be bounded (such as files or databases) or unbounded
    (such as streams).
    
    In the context of the above definition, the entity resolution task is the
    computer program doing the processing.
    What sets entity resolution programs apart is the notions they use to
    process information from a particular source.

    \begin{defn}
        An \textit{attribute} is information about an entity that has a certain
        meaning in a context given by a frame of reference and certain rules of
        interpretation.
    \end{defn}

    Attributes by themselves are not enough to describe an entity.
    For example, `red' fully describes an entity only if we are looking for
    colors.
    If we're looking for fruit, `red' is only one of several attributes that
    help describe fruit.
    For example we might be interested in `sour' or `sweet' as well as in
    `small', `medium' or `large'.

    \begin{defn}
        An \textit{entity reference} is a collection of attributes that refer
        to a real-world entity which can be formed by following an organizing
        principle (or order) of the information source where the attributes are
        all located.
    \end{defn}

    The organizing principle of an information source is simply an order that
    facilitates processing that information source.
    For example, in a CSV file it would be the rule that up until the first
    comma on each line we should find a color value.
    For operations on databases such as joins, the organizing principles is
    built around data records.
    For named entity recognition, the organizing principle is built around
    mentions.

    A good generalization of the notions in the above examples put in the
    context of entity resolution might be that of a
    \textit{reference}\cite{Ben2009Swoosh}.
    For in this context the rules for processing records, mentions or lines in
    CSV files are used simply to find references to real-world entities.

    The organizing principle of an information source shouldn't be confused with
    the structure of the information in that source.
    A single table record in a database might have a few attribute that
    reference one entity (for example a company or a person) and a few other
    attributes that reference another entity (a building).
    For entity resolution tasks that value geolocation above other things, the 
    fields that reference the building will constitute the entity reference.
    For tasks that identify people, the entity reference will be comprised from
    the attributes that reference a person or a company.
    The organizing principle of the information source by which attributes are
    collected as entity references by the entity resolution task is extrinsic to
    the information source and is somehow linked to the entity resolution task
    itself.
    We say `linked' because the organizing principle may or may not be an
    integral part of the entity resolution task.
    For instance, entity alignment tasks rely on external knowledge bases and
    entity matchers based on deep learning rely on machine learning for
    determining the organizing principle of every data source they consume.

    The other important distinction is that the attributes that make up an
    entity reference are collected from the same information source.
    This creates a clear belonging relationship between entity reference and
    information source.
    
    Let's suppose that we are looking to identify colors across multiple
    information sources.
    In one information source the colors are represented as `red', `green' or 
    `blue'.
    In another information source they are represented as `CC0000', `00CC00' or
    `0000CC'.
    Because we are looking for colors we decide that our organizing principle
    should dictate that references contain only one attribute.
    However, up until now we haven't articulated a way to tell the entity
    resolution task that.
    Namely: how do we tell the entity resolution task that it's looking for
    something that is a \textit{color}.
    Or, formulated as a question: what is \textit{color}?

    \begin{defn}
        A \textit{trait} is a semantic rule that guides the entity resolution
        task in recognizing the organizing principle of each information source
        that it uses as input.
    \end{defn}

    Every entity resolution task has goals it must accomplish.
    Some tasks stitch news articles together, others help organize content
    across social networks or deduplicate the information stored in database
    tables.
    For each task there are rules of interpreting the information at hand that
    stand out above others.
    In our case, we must simply tell our task that it should be interested only
    in colors and how those colors might look like depending on information
    source.
    
    The notion that's the most similar to that of a trait is the `feature':
    \textit{an individual measurable property or characteristic of a
    phenomenon}\cite{bishop2006pattern}.
    The mechanics of how traits and features function is near identical.
    The difference between a trait as defined here and a feature is a matter of
    perspective.
    Features are part of a more objective perspective on information.
    Traits as they are defined here are concerned with the contextual meaning of
    things.
    In the broadest sense, they are nothing more than algorithms that extract
    information according to the subjective goal of the entity resolution task.
    A trait can help the entity resolution task extract one or more attributes
    for a given entity reference that can't be inferred from the underlying
    information alone.
    For example, a trait called `profitable' is different from a physical
    representation in the form of a table column or a JSON property.
    Imagine an entity resolution task that is meant to find companies across
    databases.
    One of the traits this entity resolution task is looking for is the company
    profitability.
    In a database table, it looks at values stored in a column called `profit'.
    In a no-SQL collection, it looks at the values stored under the
    `isProfitable' property.
    Yet, the entity resolution task simply looks for the `profitable' trait in
    both these information sources to help shape the entity references it finds.

    To sum up, an attribute of an entity reference always exists in an
    information source, whereas a trait of the entity resolution task does not.
    Entity resolution tasks extract attributes that they assign to entity
    references based on the traits specific to each task.
    This point of view is supported by the need to design entity resolution
    systems that are extensible based on their sources of
    information\cite{fever2009}\cite{magellan2020}\cite{oyster2012}.

    So far we have not discussed what happens if entity references point to the
    same real-world entity.
    
    \begin{defn}
        We refer to the logical group of entity references that point to
        the same real-world entity according to the entity resolution task's
        parameters as the \textit{entity profile} of that real-world entity.
    \end{defn}

    It is important to understand that the entity profile and the real-world
    entity are not the same.
    The entity profiles are simply manifestations of the input parameters of the
    entity resolution task (namely the information sources and the traits) and
    they represent the output of the entity resolution task.
    
    By using the terms \textit{information source}, \textit{attribute},
    \textit{trait}, \textit{entity reference} and \textit{entity profile} we can
    finally define the entity resolution problem.

    \section{Problem Definition}\label{sec:problem}

    Given:
    \begin{itemize}
        \item $n \in N^*$ pairs (($S_i$, $n_i$), $1 \leq i \leq n$, $n_i \in N$), 
        where $S_i$ is an information source and $n_i$ is the number of messages
        in $S_i$,
        \item $m_i \in N^*$ traits, ($t_{ij}$, $1 \leq j \leq m_i$) for each
        information source $S_i$ so that
        \item each trait $t_{ij}$ can generate at most $a_{ij} \in N^*$
        attributes,
    \end{itemize}
    an entity resolution task $ER$ applies each trait $t_{ij}$ onto the source
    $S_i$, ($1 \leq i \leq n$, $1 \leq j \leq m_i$), in order to extract
    $r_{{S_i}k}$ entity references where
    ($0 \leq k \leq n_i \cdot \sum^{m_i}_{j=1}a_{ij}$).
    We denote with $R$ the domain of all the $r_{{S_i}k}$ entity references that
    were extracted by $ER$.
    Similarly, we denote with $P$ the domain of all entity profiles.
    The matching and clustering step of the $ER$ task is a function
    $E: R \rightarrow P$ that converts individual references to entity profiles.
    The exact definition of this function is specific for each mathematical
    model that underpins entity resolution.
    The data representation of the entity profiles in $P$ also varies according
    to the mathematical model of choice.
    On the other hand, the data representations of the concepts that are part of
    building $R$ do not differ between mathematical models.

    The most basic building block in the context of entity resolution is the
    humble attribute.
    We denote with $A$ the domain of all possible attributes that can be used by
    the task $ER$ described above.

    In this context, let $r_{{S_i}{k_j}}$ denote the $k_j$th ($1 \leq j \leq k$)
    entity reference from the source $S_i$.
    Then $r_{{S_i}{k_j}}$ is always represented as a tuple
    ($a_r$, $r \in N^*$, $a_r \in A$), $r_{{S_i}{k_j}} \in A^r$.
    Note that $r$ varies for each $k_j$.
    The concept of an order external to the information source and the manner in
    which we defined traits serves as the reason why tuples express entity
    references sufficiently well.

    A trait is then a function that helps construct entity references.
    The definition domain of this function materializes only when the $ER$ task
    extracts a given entity reference $r_{{S_i}{k_j}}$.
    The domain of this function is then the domain of all of the attributes in
    the source $S_i$ that could be meaningful in the context of constructing the
    entity reference $r_{{S_i}{k_j}}$ from a certain subjective perspective.
    We denote this domain with $A_{{S_i}{k_j}}$.
    Then the trait is defined as a function $t: S_i \rightarrow A^x$, where
    $x \in N^*$ and $x$ varies with $k_j$ as defined above for entity
    references.
    This formal definition of the trait satisfies the needs of any extraction
    process specific to entity resolution.

    During the extraction process, the entity resolution task applies multiple
    traits with the end result of building the domain of entity references:

    \[
        Ref = \bigcup_{i \in N^*,1 \leq j \leq k} r_{{S_i}{k_j}}
        \textrm{, with r, S and k specified above}
    \]

    All existing processes concerning entity resolution extraction processes can
    be adapted to this formal definition, as already noted\cite{Pap19}.
    
    At its core, an entity resolution task can then be formalized as a function:
    \[
        e : Ref \rightarrow \{P_x \mid x \in \mathbb{N}, P_x \in P \},
    \]
    
    where $P$ is the domain of entity profiles and $x$ is the number of
    resulting entity profiles.
    We denote with $Res=\{P_x \mid x \in \mathbb{N}, P_x \in P\}$ as the domain
    of all entity resolution task results.

    It stands to reason that the domain $Res$ contains a result which is the
    closest to our understanding of the real world from a given perspective in
    a given context.

    \begin{defn}
        We define the \textit{ground truth} $G$ of an input entity reference
        domain $Ref$ the entity resolution result comprised of entity profiles
        such that
        $\{P_g \mid g \in N\textrm{, $P_g$ describes a real-world entity
        completely}\}$.
    \end{defn}

    In other words the ground truth is the ideal entity resolution result for 
    given input data, but in a given context, from a given perspective.
    This result contains only those profiles that describe real-world entities.

    Now that we know what the ideal result of an entity resolution task looks
    like, we can at least formulate a measure of the task's relative qualitative
    performance with respect to the ground truth of the task's input.
    
    \begin{defn}
    Given an entity resolution task $e: Ref \rightarrow Res$ and the ground
    truth $G$ for the input entity reference domain $Ref$, a \textit{qualitative
    evaluator} is an idempotent function $q: Res \rightarrow \mathbb{R}$ that
    measures the similarity between the input of the function and $G$.
    \end{defn}

    In practice, we should strive to implement qualitative evaluators as
    \textit{pure} functions in the sense that running them on a computer will
    not entail any side-effects.

    As stated before, the shape and structure of the items in $Res$, including
    $G$, depends on the mathematical model underpining a specific entity
    resolution task implementation.
    Because the input domain of the qualitative evaluators changes according to
    the used mathematical model, the set of qualitative evaluators available for
    describing the qualitative performance of a certain entity resolution task
    $Q = \{q_y \mid y \in \mathbb{N}\}$ is dependent on the mathematical model
    used for implementing that task.

    In the strictest sense, comparing two entity resolution tasks is possible as
    long as they can be evaluated using the same set $Q$ of qualitative
    evaluators.
    Note that if the entity resolution $e_1$ can be evaluated using a set of
    qualitative evaluators $Q_1$ and $e_2$ can be evaluated using a set of
    qualitative evaluators $Q_2$ such that $Q_1 \cap Q_2 \neq \emptyset$, we
    have a set of qualitative evaluators $Q_\cap = Q_1 \cap Q_2$ that allows us
    to compare $e_1$ and $e_2$ based on $Q_\cap$.
    Future developments will seek to further loosen this constraint to allow
    comparing entity resolution tasks that do not share a set of qualitative
    evaluators.

    \begin{defn}
        Given
        \begin{itemize}
            \item the set of entity references $Ref$,
            \item a set of qualitative evaluators $Q$ that are common to all
            entity resolution tasks $e_i : Ref -> Res$, $i \in \mathbb{N}$
            \item the ground truth $G$ for the input domain $Ref$,
            \item the qualitative evaluator results $V_i=\{q_{i} \mid q_{i} = q(G, e_i), q \in Q\},
            i \in \mathbb{N}$,
        \end{itemize}   
        we define the comparison of the entity resolution tasks above as
        $C : (
            \{V_i \mid i \in \mathbb{N}\}
            \times
            \{V_j \mid j \in \mathbb{N}\},
            i \neq j
        ) \rightarrow \{-1, 0, 1\}$ such that:
        \[ 
C(V_i, V_j) = \left\{
\begin{array}{ll}
      -1,~\textrm{if $e_i$ is less fit for purpose than $e_j$}\\
      0,~\textrm{if $e_i$ is as fit for purpose as $e_j$}\\
      1,~\textrm{if $e_i$ is better fit for purpose than $e_j$}\\
\end{array} 
\right. 
\]

    \end{defn}
    
    Works that synthesize ER evaluation: (to be cited)

    \section[ere]{Entity Resolution Evaluation}\label{sec:ere}

    From a process perspective, entity resolution is usually comprised of four
    different activities\cite{Pap19,Tal11}:
    \begin{itemize}
        \item \textit{blocking} --- grouping together entities that share
        similar traits and thus have a higher chance of matching with the
        explicit purpose of narrowing down the scope for the rest of the
        activities enumerated here;
        \item \textit{filtering} --- typically refers to rooting out entities
        that have a very small chance of matching other entities
        \item matching --- the defining activity of the entity resolution
        process
        \item clustering --- results that point to the same real-world entity
        are usually grouped together in entity profiles
    \end{itemize}

    Out of the above activities, we choose to focus only on matching and
    clustering especially since the performance of filtering and blocking was
    already studied extensively\cite{Pap19}.
    Matching and clustering are the core tasks that drive the overall quality of
    an entity resolution task.
    We shall see that we can get significant hints from the mathematical models
    that underpin entity resolution tasks about which qualitative measurements
    we can use and how.
    More notably, we note that each mathematical model constrains the data
    representation that can be used by the entity resolution task.

    There are a few mathematical models that deal with entity resolution.
    They range from models based on complex networks and graph
    theory\cite{Li2020} to probabilistic and algebraic ones.
    In this paper we describe three of these models and the data structures they
    constrain the outputs of the entity resolution tasks to.

    \subsection[fsm]{Fellegi-Sunter Model}\label{subsec:fsm}

    In the late 1960s Ivan Fellegi and Alan Sunter wrote the seminal
    paper\cite{fs1969} for regarding entity resolution as a probabilistic
    phenomenon.
    To this day, this is the most popular way of describing the entity
    resolution problem from a mathematical perspective even though it is not
    a complete perspective relative to the activities that comprise the entity
    resolution process.

    In this mathematical model, entity resolution is defined in the context of
    making probabilistic decisions.
    The three decisions any entity resolution task can make are
    \textit{link ($A_1$)}, \textit{non-link ($A_3$)} and
    \textit{possible link ($A_2$)}.
    The statistical errors
    $\mu=\sum_{\gamma \in \varGamma}u(\gamma)P(A_1|\gamma)$ and
    $\lambda=\sum_{\gamma \in \varGamma}m(\gamma)P(A_3|\gamma)$ express the type
    one and type two errors when it comes to entity resolution.
    $\gamma$ represents a comparison vector that contains the results of
    comparing two entity references by for each shared trait. The function
    $u(\gamma)$ is the probability that the entity references do \textit{not}
    realize $\gamma$, while $m(\gamma)$ is the probability that they \textit{do}.
    $\varGamma$ represents the entire space of possible realizations of $\gamma$.

    In this context, entity resolution is defined as a function that assigns the
    probabilities $P(A_1|\gamma)$, $P(A_2|\gamma)$ and $P(A_3|\gamma)$ for each
    $(\mu, \lambda)$ such that $P(A_2)$ is minimal at those error levels.
    In other words, entity resolution is a task that will strive to decide with
    as little ambiguity as possible whether two entity references match or don't
    match based on their attributes and the traits of the entity resolution
    task.

    Because the Fellegi-Sunter model defines entity resolution as a statistical
    optimization function, it works very well for both
    rule-based\cite{oyster2012} and machine learning\cite{deepm2020}
    implementations.
    The qualitative performance of the implementations of this model can then be
    measured using standard statistical measures.
    With the type 1 and type 2 errors defined, the notions of true positive,
    true negative, false positive and false negative can be derived immediately.
    And with these notions, we can use measures such as precision, recall or
    the $F_1$ score.
    $$Precision=\frac{TP}{TP+FP}$$
    $$Recall=\frac{TP}{TP+FN}$$
    $$F_1=2 \cdot \frac{Precision \cdot Recall}{Precision+Recall}$$

    However, there is more to be said about this model.
    Since it only addresses \textit{matching}, this model only accounts for
    the result of comparing two entity references at a time\cite{Kon19}.
    Among other things, this means that this model does not support transitive
    linkage rules\cite{Tal11}.

    Using our terminology, the attributes will be the basic building blocks for
    how information is represented.
    In the Fellegi-Sunter model we would then represent entity references as
    tuples $t=(a_1,a_2,\ldots,a_n)$, where $n>0$.
    The result of any entity resolution task that implements this model can be
    represented as a list of pairs of tuples.
    
    \subsection[serf]{Stanford Entity Resolution Framework Model}\label{subsec:serf}

    \textcolor{green}{Please mention the model's parents and add a reference!}
    \begin{itemize}
        \item views entity resolution as the result of applying a match function, a merge function and a domination relationship on top of a set
        \item allows evaluating transitive matches
        \item the match function can be anything as long as we can act with a yes/no action on its result
        \item the merge function should be distributive (merge(a, merge(b, c)) = merge(merge(a, b), c))
        \item provides a very flexible way of modeling entity resolution (the comparison and addition of integer numbers is a valid basis for an entity resolution task on the set of integer numbers)
        \item the entity profile is either one of the original input items or an information that dominates all the information that was used to create it
    \end{itemize}

    \subsection[algebraic]{Algebraic Model}\label{subsec:algebraic}

    John R. Talburt proposes this model. 
    \textcolor{green}{add a reference!}
    \begin{itemize}
        \item ER is an equivalence relation on a set of items
        \item reflexive, symmetric, transitive
        \item ER is the same as the partition generated by a given equivalence relation over an input set
        \item very easy to work with and very comprehensive (transitivity, identity resolution, etc)
        \item the entity profile is a partition class (a cluster)
    \end{itemize}

    \section{Data Structures}\label{section:data-structures}

    Each of the above models assumes the output of an entity resolution task has a certain shape. In programming, this shape can be approximated by using certain abstract data types. If we accept that entity resolution tasks use tuples as the most basic data structure they operate on then each of the mathematical model produces more complex abstract data types that contain tuples.

    FSM produces a list of pairs of tuples (i.e. it produces matches). 
    \textcolor{green}{ar fi f utila o formalizare a acestui output}

    The algebraic model produces a partition over the set containing all the items that we are running the entity resolution task on.
    \textcolor{green}{ar fi f utila o formalizare a acestui output}

    The SERF model is very flexible and the best way we could imagine its output is a stream of tuples. 
    \textcolor{green}{ar fi f utila o formalizare a acestui output}

    What the tuples actually look like, depends on the merge function. That means that the SERF model can have the exact same output as the algebraic model. On the other hand, if we run entity resolution over a set of input items that only have string attributes, and we use a merge function that concatenates the corresponding attributes of matching items, then we end up with an output that contains tuples of the same size as the input tuples, but with different content.
    \textcolor{green}{add an example!}

    \section{Metrics}\label{section:metrics}
    
    Just as we have different data structures we can expect each mathematical model to output, we can expect to be able
    to apply different metrics depending on the mathematical model.

    \subsection[fsm]{Fellegi-Sunter Model}\label{subsec:fellegi-sunter-model}
    
    For the F-S model, we can use standard probabilistic methods that rely on the notions of true or false positive or negative (explain what a TP is in the context of ER; same for FP, FN, TN). Compound measures like the Precision, Recall, F-score, Accuracy, Sensitivity, and others can also be used. 
    \textcolor{green}{add formula for all these metrics!}


    \subsection[serf]{Stanford Entity Resolution Framework}\label{subsec:stanford-entity-resolution-framework}
    
    Given the flexibility of the SERF entity resolution model, it needs a very generic way of evaluating the quality of the entity resolution task. This measure is the Generalized Merge Distance \textcolor{green}{add a reference!}. A few other measurements can be derived from the merge distance between the ground truth and the entity resolution task's result: Variation of Information, Pairwise Precision, Pairwise Recall, Pairwise F1. 
    \textcolor{green}{add formula for all these metrics!}

    \subsection[algebraic]{Algebraic model}\label{subsec:algebraic-model}

    The algebraic model relies on partitions over the initial set of items. Therefore, we can apply clustering metrics most easily for this model: Talburt-Wang index, Rand Index, Adjusted Rand Index, Pairwise Precision, Pairwise Recall, Cluster Precision, Cluster Recall, Pairwise and Cluster F1 measure.
    \textcolor{green}{add formula for all these metrics!}

    \subsection[why-bother]{Consequences of Choosing a Model}\label{subsec:why-bother}

    We can't compare precision with pairwise precision or with cluster precision. This speaks to the fact that once we choose a mathematical model to evaluate an algorithm, we can compare the results of that algorithm only with other results obtained for the same mathematical model. Furthermore, any merge distance depends very much on the chosen merge function. The data output by the entity resolution task has a different structure based on that function. Therefore, the pairwise measures output by the SERF are not comparable to the pairwise measures output by the algebraic model. Another thing to take into account is the format in which the ground truth supplied for a standard entity resolution data set is provided. If the ground truth is provided as a list of pairs, we will have to provide the same type of data structure for the evaluation. Similar indications for ground truths provided as partitions or lists of items.

    \subsection[applying]{How to Apply Models}\label{sub-sec:applying}

    One could argue that we could translate from one ground truth to another. This is trivial in some cases, but not so in others. The translation from algebraic to SERF is trivial: the match function is the equivalence relationship whereas the merge function groups matching items within a container (a list or a tuple). The translation from the SERF model to the algebraic model can be performed only if there's a way to translate from the results provided by the entity resolution task to a partition over the set of input items. Converting from partitions to pairs of matches is also trivial. Converting from whatever SERF outputs based on the merge function to pairs might even be impossible (see concatenation example). In light of all this we argue that some algorithms simply prefer some mathematical model over others.

    \section{Identifying the Mathematical Model of an Entity Resolution Implementation}\label{section:implementation}
    
    All entity resolution tasks are very easily explained in terms of the FSM\@. However, some entity resolution tasks cannot do transitive matching, i.e.\ if a matches b and b matches c then a matches c. 
    As we shall see, those tasks can't be expressed in terms of the algebraic model or in terms of the SERF model without adapting their output to the data structure expected by those models. The entity resolution systems that are based on the Swoosh family of algorithms (citation needed), such as Oyster fall under the SERF family of models. The rest of the algorithms that provide transitive matching can be expressed primarily via the algebraic entity resolution model. Again, the obvious missing elephant in the room is graphs.

    \section{Experiment}\label{section:experiment}

    As a walkthrough for the concepts discussed thus far, we present an experiment where we use an FSM algorithm and adapt it to the algebraic and SERF models.

    \subsection[algo]{Algorithm}\label{subsec:algorithm}
    
    ppjoin - description, why is it FSM

    \subsection[data]{Data}\label{subsec:data}
    
    We generate a data set based on the Abt-Buy dataset 
    \textcolor{green}{add a ref} 
    for entity resolution provided by the University in Leipzig. We split the Abt data into two sub-datasets. We keep 2 columns
    \textcolor{green}{I would use the same terms as in section 4. Pb def.; eg: we keep two attribbutes} 
    in common between the two datasets. We assign the rest of attributes randomly between the output data sets. We do this splitting because we want to operate ppjoin in a controlled environment.

    \subsection[adapters]{Output Adapters}\label{subsec:adapters}

    PPJoin outputs a list of pairs. We develope an adapter for transforming the list of pairs to a partition over the initial set considering the list of pairs to define the equivalence relation between matching items. We implement a different adapter for transforming the list of pairs to a list of generic items by reusing a sample merge function that we provide to the SERF model.

    \subsection[method]{Methodology}\label{subsec:methodology}

    We run ppjoin at different Jaccard thresholds and note the differences between the measurements supplied by each model. 
    \textcolor{green}{The thresholds are 0.0, 0.4 and 0.7. - I would move this st into the numerical results; here is about how to do, only}     
    We interpret the results for each threshold-based model. We also discuss the differences between models for each threshold.

    \subsection[results]{Results}\label{subsec:results}

    Three values of Jaccard threshold have been investigated.
    \textcolor{green}{Please add an argument why only these three values. How did you slect them?} 

    \textcolor{green}{please add the numerical results (tables, figures)}

    We notice that for 0.0, FSM thinks the algorithm 
    \textcolor{green}{it's a little bit confusing: a model thinks that an algo do something :D; furthermore, give more details about the aim of the algorithm; it is ppjoin?} 
    performs very well, but SERF and the algebraic model give us additional information about the consistency of the results. 
    SERF thinks there's a huge distance between what we should be getting and what we're actually getting (the variation of information is also off the charts).
    The algebraic model reveals very poor cluster metrics.

    \textcolor{green}{some examples, with the correctly and wrong matched entities, could be helpful} 

    Do a similar analysis for 0.4 and 0.7.

    \section[conclusion]{Conclusions}\label{section:conclusions}

    The work speaks about how we can use mathematical models to evaluate entity resolution results. It also provides easy rules of thumb to identify the model that's best suited to interpret the results of an ER task.
    
    Future work:
    \begin{itemize}
        \item result translation between math models
        \item ground truth and data generation for ER tasks
    \end{itemize}

    \bibliographystyle{plain} % We choose the "plain" reference style
    \bibliography{er-general,er-related-work,er-additional-references}
\end{document}
