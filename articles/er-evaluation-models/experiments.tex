So far we have shown how theoretical models influence the data structures
that are available to an entity resolution task.
The question was whether this also has some perceivable outcome in an
experimental setup.
Our experiment proposes using the same entity resolution algorithm across
different data sets to explore whether there are some invariant conditions
that hold true regardless of the data set.

The code for running the experiment is available on GitHub~\cite{matchescu}.
Running the experiment does not require any specific hardware.
We use the Python~\cite{python} programming language (version 3.11) and the
Numpy~\cite{numpy} and Pandas~\cite{pandas2023} libraries for efficient
data manipulation.
All metrics are computed using an OpenSource library available on GitHub as
well~\cite{matchescu-er-metrics2023}.

\subsection{Datasets}\label{subsec:data}

All experiment datasets are available on GitHub\cite{expdata2023}.
To ensure we are working with standard data that simulates real data and which
is widely used and therefore familiar, we use the Abt-Buy, DBLP-ACM and
Amazon-GoogleProducts~\cite{vldb2010} data sets.
For small scale experiments we are using our own control
dataset~\cite{expdata2023} which we create from existing data.
To ease experimentation, we use the original tabular representation
which allows us to take advantage of Pandas~\cite{pandas2010,pandas2023}
and also has the benefit of not requiring adaptation to an internal format.
The tabular data is stored on disk as CSV files.
Each CSV file has a first row containing the column headings and subsequent
rows containing records within which the fields are separated by comma.
The data representation, however, is not important for the experiment's
outcome.

\textcolor{green}{
E nevoie si sa se precizeze dimensiunea aceastor seturi de date (cate exemple
sunt, cate features sunt)
}
\textcolor{orange}{
    Pare repetitiv sa adaug aici informatia care este publicata in tabelul
    "Table 1. Overview of real-world evaluation match tasks"
    din sursa citata.
    Am facut mai clar lucrul ca informatia e acolo, dar nu stiu daca e ok.
    Daca nu, pot face un tabel si aici.    
}

% The Abt-Buy dataset is an e-commerce data set which consists of two collections
% of entity references: Abt and Buy.
% Abt contains 1081 entity references from abt.com, each having the following
% attributes: id, name, description, price.
% Buy contains 1092 entities from buy.com with the same attributes as the entity
% references in Abt plus one additional attribute: manufacturer.
% The ground truth has 1097 matching pairs of entity references.

% DBLP-ACM is a bibliographic dataset and comprises two entity reference
% collections: DBLP2 and ACM.
% DBLP2 contains 2616 entity references with the following attributes: id, title,
% authors, venue and year.
% ACM contains 2294 entity references, with the same attributes as the entity
% references in DBLP2.
% The ground truth contains 2224 matching pairs of entity references.

% Another e-commerce dataset, Amazon-GoogleProducts holds two entity reference
% collections: Amazon and GoogleProducts.
% Amazon contains 1363 entity references with the following attributes: id, title,
% description, manufacturer and price.
% GoogleProducts contains 3226 entity references, with the same attributes as the
% entity references in the Amazon subset.
% The ground truth contains 1300 matching pairs of entity references.
% The particularity of the Amazon-GoogleProducts dataset is the use of textual
% values even for the id attribute values.

We use the benchmark datasets almost in their original forms and the ideal
mapping to establish the ground truth for each dataset.
The sizes, the attributes that might match as well as the information about the
ideal pairing of entity references can be found in the original
paper~\cite{vldb2010}.

In order to make entity matching algorithms that use prefix matching, we make a
slight adjustment to the benchmark datasets and move the `id' column to the last
position in each data set.

With our custom dataset, the goal is to exert complete control over the entity
resolution process.
This is achieved by creating custom input data sources for the entity resolution
process through data generation.
The data sources each contain ten chosen items from the `Buy' subset of the
`Abt-Buy' dataset.
The chosen items exacerbate faults in data.
There are many empty or missing attribute values and the data is full of
near-duplicate entity references.

The data generation procedure divides the ten-item subset into two distinct
tables:

\begin{enumerate}[label=\textbullet,leftmargin=1cm]
\item DG1, with columns: \texttt{`name', `manufacturer', `price', `id'}
\item DG2, with columns: \texttt{`description', `name', `id'}
\end{enumerate}

These tables serve as the foundational input for our experimental analysis.
The construction of the ground truth, as a list of paired elements, is
straightforward due to this data generation approach.
We operate under the assumption that each item in our `Buy' subset
corresponds uniquely to a single real-world entity.
Consequently, the ground truth consists of pairs, each pair representing a
distinct real-world item.

The experiment is set up so that in the data extraction phase that precedes the
actual entity resolution process, the data undergoes uniform conversion to
string format, emphasizing word extraction.
This approach is consistent with both the benchmark and the generated data sets.
The pipeline ensures a left-to-right, top-to-bottom arrangement of extracted
words, with column and row order preservation, thus providing the entity
resolution task references consisting of a single text attribute.
This attribute amalgamates words from all attributes in the source data records
into a cohesive text unit per record.
We employ this process to effectively ensure sequence neutrality.

Let us take the following record from the `Buy' subset as an example.

\begin{table}[ht]
    \setlength\tabcolsep{5pt}
    \begin{tabular}{|l|l|l|l|l|}
        \hline
        id&name&description&manufacturer&price\\
        \hline
        205554724&Seiko SXDA04& & &\$138.00\\
        \hline
    \end{tabular}
    \label{tab:buy-record}
    \caption{An example record of the 'Buy' subset}
\end{table}

The following entry corresponds to the record in Table \ref{tab:buy-record}.

\begin{table}[ht]
    \setlength\tabcolsep{6pt}
    \begin{tabular}{|l|l|l|l|}
        \hline
        name & manufacturer & price & id \\
        \hline
        Seiko SXDA04 & & \$138.00 & 205554724 \\
        \hline
    \end{tabular}
    \label{tab:dg1-record}
    \caption{TBA!}
\end{table}

The corresponding DG2 record would then look as follows.

\begin{table}[ht]
    \setlength\tabcolsep{5pt}
    \begin{tabular}[b]{|l|l|l|}
        \hline
        description&name&id \\
        \hline
        &Seiko SXDA04&205554724 \\
        \hline
    \end{tabular}
    \label{tab:dg2-record}
    \caption{TBA!}
\end{table}

Using the extraction traits described before would yield us the
following entity references from DG1 and DG2, respectively:
\begin{enumerate}
    \item \texttt{(`Seiko',`SXDA04',`\$138.00',`205554724')},
    \item \texttt{(`Seiko',`SXDA04',`205554724')}.
\end{enumerate}

We will refer to our generated dataset with the `mini-buy' moniker.

\subsection{Entity Resolution Algorithm}\label{subsec:entity-resolution-algorithm}

In searching for invariant conditions that hold true regardless of the data set
that we use for experimentation, it is tempting to say that the choice of entity
resolution algorithm is non-consequential.
Given that this experiment is our first foray into the matter, it seems wise not
to give in to temptation.
There are two limitations that we find important in choosing an appropriate
entity resolution algorithm.

Firstly, the algorithm should perform reasonably well without losing the ability
to easily explain its outcome.
In this context we are thinking primarily about quantitative performance marks
such as resource consumption or time spent.

Secondly, we want to choose an algorithm that has very few configurations so
that its qualitative performance is easy to plot over a relatively large  number
of configurations.

The \texttt{ppjoin}\cite{ppjoin} entity matching algorithm meets these criteria.
PPJoin stands for Position Prefix Join.
It is an algorithm designed to find similarities between records by comparing
them using a prefix of each record determined by employing the Jaccard
index~\cite{jaccard1912,finley1996}.
The algorithm's outcomes are intuitively easy to explain.
A lower Jaccard threshold will cause the algorithm to take into account shorter
prefixes, thus increasing the chances of a match.
A higher threshold should match fewer items, but with less chance for error.
Throughout the experiment, the Jaccard threshold will be denoted with \textit{t}.

Our experiment consists of the following steps:

\begin{itemize}
    \item generate or load the data;
    \item take 100 entity matching passes through the input data with
          Jaccard thresholds ranging from 0 to and including 0.99 in steps of
          0.01, storing the results for each step;
    \item compute the statistical and algebraic similarity metrics for each
          of the stored results.
\end{itemize}

Similar experiments were performed in the past~\cite{draisbach2013choosing}.
Those experiments were looking at the relationship between the size of a dataset
and a threshold used for configuring a similarity measure.
A threshold that is a configuration of a similarity measure plays a different
role than the role of the Jaccard threshold in the context of \texttt{ppjoin}
even though both thresholds limit the amount of data being compared during the 
match phase of the entity resolution process.
The Jaccard threshold as used in \texttt{ppjoin} acts as an influence on the
fundamental probabilities defined by the Fellegi-Sunter model, actively
manipulating the number of attributes that take part in the matching phase of
the entity resolution process.
On the other hand, configuring a threshold for a similarity measure affects the
comparison being performed at the level of each attribute value.
The number of attributes that take part in the comparison remains unchanged.

In contrast, our experiment uses multiple benchmark datasets of similar size and
different makeup.
Our control dataset is, indeed, small but we are not interested in its size, but
in its makeup.
The purpose of our experiment is to find invariants (perhaps other than dataset
size) which could prove useful in analysing entity resolution quality.
To this end we want to examine the effect of simply changing the looking glass
through which we inspect entity resolution results.

The \texttt{ppjoin} algorithm treats entity resolution as a matching problem.
Consequently, Fellegi-Sunter is the most fitting entity resolution model.
Therefore, in order to evaluate the quality of the algorithm under the algebraic
model, we have to convert a list of matches (the data structure output by the
 \texttt{ppjoin} algorithm) to a partition over a set.

For our own generated data we are able to generate the ground truth partition
for the algebraic model based on the input data.
For the benchmark data sets we convert the ideal mapping to a partition over the
input data using the algorithm presented in Subsection~\ref{subsec:fsm-alg}.

\subsection{Outcomes from Generated Data}\label{subsec:experiment-mini-buy}

\subsubsection{Fellegi-Sunter Model Results}

The ground truth and the results are represented as lists of pairs.
The ground truth was generated by iterating over both input data sources using
the same cursor and outputting pairs of records.
Note that there are duplicate CSV records that only differ on the `id' column.

Intuition tells us that for values of the Jaccard similarity threshold
\textit{t}\footnote[1]{the Jaccard similarity coefficient is used to determine
the length of the prefix which should be used to compare two entity references
to determine whether they refer to the same entity or not} that are either too
low or too high we should have lower precision.
For higher thresholds we should also have lower recall values, whereas for
lower values the recall should be higher.
Figure~\ref{fig:mini-buy-fs} shows the \texttt{ppjoin} results at various
values of the Jaccard threshold $t$.

\begin{figure}[!hp]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=\columnwidth]{mini-buy-fsm-main}
    \caption{Statistical metrics on `mini-buy' dataset
    \textcolor{green}{for results of F-S model}
    }
    \label{fig:mini-buy-fs}
\end{figure}

We can make some observations based on Figure~\ref{fig:mini-buy-fs}.
For values of $t \geq 0.78$, we end up with fewer (lower recall), but more
accurate matches (higher precision) than compared to lower values of $t$ because
the amount of false positives decreases and the amount of false negatives
increases.

For $0.6 \leq t \le 0.78$, precision decreases because the false
positive rate increases and recall increases because the false negative rate
decreases.
This is in line with expectations: the prefix becomes shorter causing a higher
percentage of attributes to be identical.
However, the prefix length is not short enough so that some of the matching
entity references in the ground truth that are more different from one another
will also be captured by \texttt{ppjoin}.

Compared to the previous interval, for values of $0.36 \leq t \le 0.6$, both
precision and recall increase compared to the previous interval.
As the prefix drops, more and more items from the ground truth are matched.
The number of false negatives decreases while the number of false positives
does not increase as fast as the number of true positives.

When $t$ is nearing zero there is an expected increase in recall due to fewer
and fewer overall negatives.
Precision, expectedly decreases due to an ever higher false positive rate.

We need to make a small note on near-identical items in the control data set,
such as \texttt{208114672} and \texttt{208114673}.
For lower values of \textit{t}, the algorithm produces two true positives and
two false positives.
By itself, the increase in false positives intuitively lowers precision.
However, lower values of \textit{t} also broaden the comparison space which
now contains entity references with short values for the name attribute,
like \texttt{205554724}.
By having new items to compare, we also increase the amount of true positives.
This dynamic ends up increasing precision as we lower values of \textit{t} so
long as we are not operating on the full input set.

Note that while we do describe the dynamic specifically for the \texttt{ppjoin}
algorithm, the dynamic of having an increased number of true positives offset
precision in an unexpected direction is not algorithm specific at all.
Regardless of the algorithm that we use, given some circumstances specific for
that algorithm, it should be possible to obtain the same effect we see here.
This observation is interesting because most entity resolution users look at the
F1 score thinking that it provides a good balance between precision and recall.
These users rarely analyse the components of the F1 score.
This situation can lead to unexpected outcomes.
For instance, there might be a substantial drop in F1 scores, which are
indicators of accuracy, when the input data for the entity resolution process is
changed.
The reason for this is that the precision or recall may be higher than expected
for the ideal configuration determined in the controlled scenario.

On the other hand, a balanced trade-off between precision and recall might not
be a requirement.
For instance, a core legal principle applicable in many jurisdictions across the
world states that is far more favourable to let 100 guilty people escape than to
put one innocent behind bars.
By that logic, we only care about precision.
Perfect recall while still retaining some precision might be useful during
research (e.g~finding papers on the same topic) or medical diagnosis (other
cases with the same symptoms).
In these cases, sifting through false positives might be preferable to missing
out on important information because of a false negative that is due to a
suboptimal input configuration.

Our control dataset (which is purposefully flawed) instructs us that the ideal
configuration values for our algorithm are:
\begin{itemize}
    \item $0.15 \leq t \leq 0.38$ for the best balanced outcome
    \item $t \leq 0.38$ for the most sensitive system
    \item $0.76 \leq t \leq 0.95$ for the highest precision
\end{itemize}

\subsubsection{Pairwise Metrics}

The ground truth and the result of the ER process are represented as partitions
over an input set of data.
Because we include the IDs in both DG1 and DG2, the input data set will
contain 20 items.
The ground truth contains 10 pairs of items.

Measuring the similarity of two partitions can be accomplished in more ways than
measuring the statistical success of a matching process.
Among these metrics, the pairwise comparison of two partitions is the best
approximation of the statistical metrics\cite{Men10}.
Figure~\ref{fig:mini-alg-pairwise} shows the computed pairwise metrics for
various values of \textit{t}.

\begin{figure}[!h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=\columnwidth]{mini-buy-algebraic-pairwise}
    \caption{Pairwise metrics on `mini-buy' dataset
    \textcolor{green}{for results of algebric model}\\
    \textcolor{green}{oare in legenda, la verde, nu ar tb sa fie pairwise F1?}
    }
    \label{fig:mini-alg-pairwise}
\end{figure}

We can see a plot similar to the one in Figure~\ref{fig:mini-buy-fs}.
The optimal range for high PF1 scores is the same to the case of statistical measures and the pairwise precision plot's shape
is almost identical (save for a smoother in the statistical model).

It is in terms of recall where we see a major difference from the statistical
model's plot.
More specifically, for $t \le 0.12$ pairwise recall drops significantly whereas
statistical recall stays at the maximum value.
The reason behind this variation lies in the difference between the two models of entity
resolution.
Whereas the standard recall formula only accounts for false negatives (of which
we can not have any using the generated data set), pairwise recall requires the
result data to be partitioned.
The partitioning operation removes any duplicates a partition class might
contain (such as duplicate matches returned by \texttt{ppjoin} for very low
values of \textit{t}).
Probabilistic recall seems to fail to adequately address situations where the
volume of data returned at the end of the entity resolution process vastly
exceeds the size of the ground truth, despite encompassing all true positives.
While in the context of a control dataset this may seem like an unlikely issue,
consider that for practical applications the size of the ground truth is often
several orders of magnitude smaller than the size of the input.

We note again that while the circumstances in which we show this behaviour are
specific to \texttt{ppjoin}, the phenomenon in itself is specific to the model
we employ when we evaluate the algorithm's performance.
Other algorithms may provide the conditions for this phenomenon using their own
input configurations.
To ensure a valid measurement system (one that is both accurate and precise), we
should collect pairwise metrics along with statistical metrics.

\subsubsection{Cluster Metrics}

Pairwise metrics are a form of measuring how well-formed the output of an entity
resolution task is.
The other widely spread family of metrics from this category that resembles
statistical metrics are the cluster metrics, which are displayed in
Figure~\ref{fig:mini-alg-cluster} for 'mini-buy' dataset.

\begin{figure}[!h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=\columnwidth]{mini-buy-algebraic-cluster}
    \caption{Cluster metrics on `mini-buy' dataset 
    \textcolor{green}{for results of algebric model}\\
    \textcolor{green}{oare in legenda, la verde, nu ar tb sa fie cluster F1? (stiu ca e sisnonim cu CCM, dar cand s-a dat formula a fost botezata CF1, iar in prima figura s-a facut referire tot la F1)}
    }
    \label{fig:mini-alg-cluster}
\end{figure}

It is refreshing to see that the optimal value of \textit{t} for a well-balanced
output should still be in the $\left[0.15,0.38\right]$ interval.
We also notice that cluster metrics point out the lack of recall when \textit{t}
nears zero even more than the pairwise metrics.
A more nuanced point is that recall decreases for $t \in \left[0.38,0.58\right]$
much more pronouncedly.

Clustering metrics also reveal interesting aspects about the other metrics we have
seen so far.
For example, we expect increased statistical and pairwise precision to
correspond to increased cluster precision.
This is only somewhat true.
As \textit{t} decreases, we see statistical and pairwise precision increasing
along with cluster precision.
What is interesting is just how much cluster precision increases.
As \textit{t} decreases, more and larger clusters are returned by the entity
resolution task.
For a series of values of \textit{t} the number of returned clusters does not
change much, while the shape of the partition returned by the entity resolution
task becomes more and more similar to the shape of the ground truth.
As the entity resolution task starts returning larger and larger clusters, the
cluster precision drops significantly even though the number of clusters
returned by the entity resolution task drops as well.

A similar effect can be observed for cluster recall as we increase the value of
\textit{t}.
When statistical precision increases, there is a good chance that the number of
clusters in the entity resolution result which are in agreement with the ground
truth would also increase.
Because there are fewer matches as statistical precision increases, we have a
partition that contains many singletons.
This causes cluster precision to decrease (because it relates to the number of
clusters returned by the entity resolution task) and cluster recall to increase
(because it relates to the number of clusters in the ground truth which remains
constant).

These phenomena do not depend on the entity resolution algorithm, but are
characteristics of cluster precision and cluster recall.
Higher statistical recall will always mean fewer and larger clusters some of
which will overlap with clusters in the ground truth.
Higher statistical precision will always mean more singleton clusters returned
by the entity resolution task which, in turn, will increase cluster recall.

Remembering that we might not care about balanced output, we can consider
cluster precision and cluster recall as metrics that balance out the bias built
into statistical precision and recall.
Furthermore, cluster precision and recall can also be used together with
pairwise metrics to confirm any blind spots in the statistical metrics.

In our own practical example, we will use these findings to keep an eye on
$t\in\left[0.76,0.81\right]$ as an interesting input configuration when we are
biased towards better precision.

\subsubsection{Clustering Indexes}

Even though pairwise and cluster metrics complement the statistical metrics,
sometimes all we want is a clear picture of the optimal input configurations for
the algorithm.
The easiest way to glean this information is by using one of the clustering
indexes depicted in Figure~\ref{fig:mini-alg}.

\begin{figure}[!h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=\columnwidth]{mini-buy-algebraic-main}
    \caption{Clustering indexes on `mini-buy' dataset}
    \label{fig:mini-alg}
\end{figure}

We observe that the plot of the Adjusted Rand Index indicates both the desirable
and the undesirable values of \textit{t}.
Notice that it ranks $t\in\left[0.76,0.81\right]$ almost as highly as
$t\in\left[0.15,0.38\right]$.
By following its plot, we should stay clear of values of \textit{t} from other
intervals --- especially those at the extremities of the definition interval.

By comparison with the ARI, the Rand Index itself is not very informative.
It signals that recall is not perfect for values of \textit{t} nearing zero and
does give the highest score for $t\in\left[0.76,0.81\right]$.
However, it also gives high scores for values of \textit{t} where all the other
metrics do not.

The Talburt-Wang index's plot is a fairly good indicator, taking all our former
observations into account.
The only aspect where it is lagging is for $t\in\left[0.76,0.81\right]$.
Here it fails to clarify the favorable circumstances due to high statistical
precision and high cluster recall.

The Adjusted Rand Index or, if operational performance is key, the Talburt-Wang
Index provide enough meaningful information to find ballpark figures for ideal
configuration settings for an entity resolution task.

\subsection{Outcomes from Benchmark Datasets}\label{subsec:experiment-benchmark}

So far we have seen evidence on the generated miniature dataset that each
theoretical model provides a lens through which we can interpret various aspects
of an entity resolution task's qualitative performance.
Some invariants with respect to the entity resolution task have been discovered.
Nothing related to the task that is being evaluated determines these phenomena.
However, we don't know if any of them are invariant with respect to the data
being used.

We want to understand if there are conditions or phenomena that could help us
extract universally good input configurations for an entity resolution algorithm
by using a small dataset.
There already seem to exist some interesting relationships between metrics
including a blind spot in how statistical recall is computed regardless of the
used entity resolution algorithm.
Now we have to see if any of them persist if we change the data used for
performing entity resolution.

In order to do this we shall use benchmark datasets.
The first of these is the `Abt-Buy' data set~\cite{vldb2010} and the plots for
our experiment are available in Figures~\ref{fig:abt-buy-fsm-main},
~\ref{fig:abt-buy-algebraic-pairwise},~\ref{fig:abt-buy-algebraic-cluster}
and~\ref{fig:abt-buy-algebraic-main}.

\begin{figure*}[ht]
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{abt-buy-fsm-main}
        \caption{Abt-Buy statistical metrics.}
        \label{fig:abt-buy-fsm-main}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{abt-buy-algebraic-pairwise}
        \caption{Abt-Buy pairwise metrics.}
        \label{fig:abt-buy-algebraic-pairwise}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{abt-buy-algebraic-cluster}
        \caption{Abt-Buy cluster metrics.}
        \label{fig:abt-buy-algebraic-cluster}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{abt-buy-algebraic-main}
        \caption{Abt-Buy clustering indexes.}
        \label{fig:abt-buy-algebraic-main}
    \end{minipage}
\end{figure*}\label{abt-buy}

We can see that the statistical model still has a pronounced bias towards higher
recall values.
The other invariant that also seems to hold for this dataset as well is the
balancing/confirmation action of the cluster metrics with respect to the
statistical/pairwise metrics.
Another nice confirmation is that the pairwise recall is never perfect.
Lastly, the clustering indexes are good approximates of the ballpark where
we could find  interesting input configurations.
All interesting values of \textit{t} 
\textcolor{green}{I would indicate the value or the value's range for this optimal $t$} 
are in the ballpark indicated by these
indexes.

On the other hand, we can clearly see that input configurations 
\textcolor{green}{la ce anume te referi prin aceste "input configs"? Ai mai amintit si mai devreme, dar nicaieri nu e specifica clar la ce anume se refera; e vorba doar de pagrul $t$ sau si de algoritm de ER si alti params?}  
such as those
that take advantage of a high precision to high cluster recall correlation are
dataset specific.

We move on to the `Amazon-Google Products' data set~\cite{vldb2010} and observe
the results in Figures~\ref{fig:amazon-googleproducts-fsm-main},
\ref{fig:amazon-googleproducts-algebraic-pairwise},
\ref{fig:amazon-googleproducts-algebraic-cluster} and
\ref{fig:amazon-googleproducts-algebraic-main}.

\begin{figure*}[h]
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{amazon-googleproducts-fsm-main}
        \caption{Amazon-Google statistical metrics.}
        \label{fig:amazon-googleproducts-fsm-main}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{amazon-googleproducts-algebraic-pairwise}
        \caption{Amazon-Google pairwise metrics.}
        \label{fig:amazon-googleproducts-algebraic-pairwise}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{amazon-googleproducts-algebraic-cluster}
        \caption{Amazon-Google cluster metrics.}
        \label{fig:amazon-googleproducts-algebraic-cluster}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{abt-buy-algebraic-main}
        \caption{Amazon-Google clustering indexes.}
        \label{fig:amazon-googleproducts-algebraic-main}
    \end{minipage}
\end{figure*}\label{amazon-google}

The plotted data shows that the statistical model is biased toward high recall
in this data set, also.
It also shows that pairwise metrics show lower score for recall while
maintaining the shape of the plot for statistical metrics.
Albeit less clearly, we still see that cluster precision and cluster recall can
still act as a balancing weight or confirmation for the statistical metrics or
pairwise metrics, respectively.

On the other hand, we start seeing that the clustering indexes are no longer
individually indicating a ballpark correctly.
The Talburt-Wang index seems to indicate the same optimal input configuration as
the cluster metrics, whereas the Adjusted Rand Index seems to provide a ballpark
of optimal input configurations that would also maximize scores obtained for
pairwise metrics and statistical metrics.
Even though one can assume that this is because of how the two indexes are
defined (TWI counts agreements on whole clusters while ARI counts agreeing
pairs), confirming this assumption experimentally requires further work.
Regardless, we can safely conclude that whether clustering indexes reveal
ballparks for optimal input configurations is dataset dependent.
\textcolor{green}{mie nu mi-e clar ce ai vrut sa spui :(}

Finally, we look at the `DBLP-ACM' benchmark dataset~\cite{vldb2010}.
The plots we obtained after running the experiment on this dataset are available
in Figures~\ref{fig:dblp2-acm-fsm-main},
\ref{fig:dblp2-acm-algebraic-pairwise},
\ref{fig:dblp2-acm-algebraic-cluster} and
\ref{fig:dblp2-acm-algebraic-main}.

\begin{figure*}[h]
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{dblp2-acm-fsm-main}
        \caption{DBLP-ACM statistical metrics.}
        \label{fig:dblp2-acm-fsm-main}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{dblp2-acm-algebraic-pairwise}
        \caption{DBLP-ACM pairwise metrics.}
        \label{fig:dblp2-acm-algebraic-pairwise}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{dblp2-acm-algebraic-cluster}
        \caption{DBLP-ACM cluster metrics.}
        \label{fig:dblp2-acm-algebraic-cluster}
    \end{minipage}
    \begin{minipage}{0.24\textwidth}
        \centering
        \includegraphics[width=\textwidth]{abt-buy-algebraic-main}
        \caption{DBLP-ACM clustering indexes.}
        \label{fig:dblp2-acm-algebraic-main}
    \end{minipage}
\end{figure*}\label{dblp2-acm}

Looking at the plots for this last dataset we again see confirmation that recall
in the statistical model is much higher than advertised by any of the other
metrics.
The other condition that is invariant with respect to the algorithm and the
data sets is that cluster precision and cluster recall act as good balancing or
confirmation metrics for the statistical and pairwise metrics, respectively.
\textcolor{green}{mie nu mi-e clar ce ai vrut sa spui :(}

We also see that the conjecture concerning the TWI as a good approximation for
input configurations that yield high clustering scores holds for this data set,
too. 
The same can be said about the ARI for approximating high scores with regard to
matching.
These observations hold for all four datasets we have experimented on.


\textcolor{green}{As incerca sa le enunt din nou acest concluzii, iar dupa ce sunt formulate le-as transforma in ipoteze, descriindu-le la inceputul sectiunii cu experimentele; astfel, ele vor reprezenta motivatia: avem 3-4 hipoteze si ne propunem sa le validam prin experimentele efectuate . Eu am retinut asa: \\
1. hypothesis about precision - TBA \\
2. recall in the statistical model is much higher than advertised by any of the other metrics.\\
3. cluster precision and cluster recall act as good balancing or
confirmation metrics for the statistical and pairwise metrics, respectively.\\
4. TWI is a good approximation for input configurations that yield high clustering scores.\\
5. ARI - ???\\
6. ???
}

