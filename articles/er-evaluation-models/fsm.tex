In the late 1960s Ivan Fellegi and Alan Sunter wrote the seminal
paper\cite{fs1969} for what was called record linkage and would later become
known as entity resolution.
To this day, the probabilistic model for entity resolution is this problem's
most popular formalization.
In this mathematical model, entity resolution is a function that aids in 
probabilistic decision making.

\subsubsection{Description}\label{subsubsec:F-S Model Description}

The original Fellegi-Sunter (F-S) model defines record linkage as an operation over two input
sets, $A$ and $B$.
The result of the operation is a set containing pairs of items
$X = \{(a, b), a \in A, b \in B\}$ or $X = A \times B$.

The model then deconstructs $X$ to two disjoint subsets $X = M \cup U$:
\begin{itemize}
    \item $M$ for pairs that contain matching items, and
    \item $U$ for pairs that contain non-matching items,
\end{itemize}
or more formally:
\begin{align}
    M &= \{(a, b) | a == b, a \in A, b \in B\}~\textrm{and}\nonumber \\
    U &= \{(a, b) | a \neq b, a \in A, b \in B\}\nonumber
\end{align}

The model proposes that if $a \in A$ and $b \in B$ are two vectors, their
comparison will also be a vector denoted $\gamma \in \varGamma$, where
$\varGamma$ denotes the set of all possible values of $\gamma$.
The comparison between $a$ and $b$ is performed between each corresponding
element in each of the two vectors.
The elements of $\gamma$ vary according to the type of comparison that is
performed\cite{winkler1990}.

In this context, a match decision function can make one of three decisions
regarding a pair from $X$ according to the F-S model:

\begin{itemize}
    \item consider it a \textit{link ($A_1$)} between the items;
    \item consider it a \textit{non-link ($A_3$)} between the items, and
    \item leave things undecided, thus marking it as a \textit{possible link
          ($A_2$)}.
\end{itemize}

Then a linkage rule is a function $L:\varGamma \rightarrow D$,
$D=\{d(\gamma)\}$, where

\begin{align}
    d(\gamma) = &\{P(A_1|\gamma),P(A_2|\gamma),P(A_3|\gamma)\}\textrm{, so
    that:}\nonumber\\
    &\sum_{i=1}^{3}P(A_i|\gamma) = 1\nonumber, \gamma \in \varGamma\nonumber
\end{align}

In other words, given a $\gamma \in \varGamma$, we have the linkage rule
\[
    L(\gamma) = \{P(A_1|\gamma), P(A_2|\gamma), P(A_3|\gamma)\}
\]
so that the probabilities sum up to $1$.

The F-S theorem defines it as the linkage rule that minimizes
$P(A_2|\gamma)$.
In other words, the optimal linkage rule according to the F-S model is the
linkage rule without uncertainties.

\begin{defn}
    The optimal linkage rule according to the F-S theorem is the entity
    resolution for two given input populations.
\end{defn}


This inclination towards optimization makes the model well suited for both
rule-based\cite{oyster2012} and machine learning\cite{deepm2020}
implementations.

The model accentuates two conditional probabilities:

\begin{align}
    m(\gamma)&=P(\gamma(a, b) | (a, b) \in M)~\textrm{and}\nonumber\\
    u(\gamma)&=P(\gamma(a, b) | (a, b) \in U)\textrm{.}\nonumber
\end{align}

\noindent
$m(\gamma)$ is the probability of a $\gamma$ comparison vector given an
$(a, b)$ \textit{link}.
$u(\gamma)$ is the probability of a $\gamma$ comparison vector given an
$(a, b)$ \textit{non-link}.
For brevity, we have not used the complete notation pertaining to records
from the original paper.

The two probabilities outlined above are important because they help us
express the probabilities of the Type I and Type II statistical errors
associated with the decision function that sits at the core of the linkage
rule definition above.
The probabilities of the two types of error are expressed as:

\begin{align}
    \mu&=\sum_{\gamma \in \varGamma}u(\gamma)P(A_1|\gamma)\textrm{,~and}\nonumber\\
    \lambda&=\sum_{\gamma \in \varGamma}m(\gamma)P(A_3|\gamma)\nonumber
\end{align}

where:

\begin{itemize}
    \item $\mu$ represents the Type I error of items that were erroneusly
    linked (i.e pairs in $M$ that do not belong in $M$), and
    \item $\lambda$ represents the Type II error of items that were
    erroneously \textit{not} linked (i.e~pairs in $U$ that do not belong in
    $U$).
\end{itemize}

While the model shows that the classification of pairs in link, non-link and
probable link is optimal in terms of statistical power, sound criticism of the
model was made in literature~\cite{tancredi2011fsmcrit}.
The criticism centers mostly around the assumptions of probabilistic
independence between various events described in the model~\cite{winkler2014matching,tancredi2011fsmcrit}.
While the criticism is constructive and leads to substantial improvements added
to the original model, the substrate of the improved models remains a
probabilistic one.
The quality metrics employed to determine the fitness of an entity resolution
result remain the same as those granted by the original model.

In addition to improving the model's fundamentals, work was done to address some
known limitations, such as the constraint of the model which limits the amount
of input populations to two~\cite{sad2013genfsm,Kon19}.
The assumption that the input data sources don't contain duplicates within
them~\cite{fs1969,sad2014fsmdup} only compounds risk that this assumption does
not hold true the more input data sources the model allows.
Then there is the issue of entity resolution transitivity: if data \textit{A}
and \textit{B} refer to the same entity, and data \textit{A} and \textit{C}
refer to the same entity then they all refer to the same entity~\cite{Tal11}.
For these two reasons and to be able to focus on the essential tools provided to
us by the probabilistic model of entity resolution, we will not pursue the
derived probabilistic models that can handle more than two input data sources. 

\subsubsection{Terminology Mapping}\label{F-S Terminology Mapping}

To link the Fellegi-Sunter model to our own terminology, let's start by talking
about `$a \in A$' and `$b \in B$'.
The F-S model calls $A$ and $B$ ``populations'' and $a$ and $b$,
``population elements''.
A distinction is made between records, denoted with $\alpha(a)$ and
$\beta(b)$, and population elements.
Most of the original paper refers to records.
Records seem to reflect what the computer knows during the entity resolution
process, whereas $a$ and $b$ seem to be things a human would know and would be
present in the input data sources.
In short, population elements describe \textit{entities} and records are
\textit{entity references} in our terminology.
We can think of the mappings $\alpha(a)$ and $\beta(b)$ as \textit{traits} in
our terminology.
The $A$ and $B$ populations equate to data sources.
A ``population'' is a particular type of data source because it is defined
as a set.

The Fellegi-Sunter model works under the assumption that there are two
populations $A$ and $B$ and defines the linkage rule as a function that uses
conditional probabilities that involve both populations.
On the other hand, our $Ref$ domain is solitary.
Using our terminology we would denote $A$ with $S_1$ and $B$ with $S_2$.
We start constructing our $Ref$ domain by using traits to extract $n_1$
entity references from $S_1$ and $n_2$ entity references from $S_2$.
Then $Ref = \{r_{{S_i}{k_j}},~1 \leq k_j \leq n_i, i \in {1, 2} \}$.
If, say there is 5 elements in $S_1$ and 10 elements in $S_2$ then $k_1$ goes
from 1 to 5 and $k_2$ goes from 1 to 10.

In this perspective, the extraction process outputs the adnotated set equivalent
to $Ref$, where the adnotations mark the original source (in the sense of $S_i$)
of the entity reference.

The result domain is expressed as $X = A \times B$ in this model.
It is subsequently split into $M$ and $U$ the sets of matching and
non-matching pairs in $X$, respectively.

The $X$ in the original model is equivalent to $Res$ in our terminology.
Since $Res$ is always a collection of entity profiles according to
Section~\ref{sec:Entity Resolution Formalization}, we must simply formalize the definition
of the entity profile domain \textit{P} as a set of pairs of entity references:
\begin{align}
    P = &\{(r_{{S_i}{k}}, r_{{S_j}{l}})\mid~1 \leq k \leq |S_i|,
    1 \leq l \leq |S_j|\}\textrm{, so that}\nonumber\\
    &i \neq j, i,j \in \{1, 2\}\textrm{,}\nonumber
\end{align}
where $|S_i|$ denotes the number of entity references extracted from the
source $S_i$.

With this specification, the $Res$ domain is comprised of sets of entity
profiles that are the result of performing the matching and clustering steps of
the entity resolution process.
The two steps are performed simultaneously because clustering is the same as
matching in this model.
That leads to representing entity profiles as pairs of entity references under
the original F-S model.

For example, let's say $r_{11}=(1, 2, 3)$ and $r_{21}=(4, 5, 6)$ are two entity
references that refer to the same real-world entity.
Under this model an entity profile would be represented as the following pair:
$((1, 2, 3), (4, 5, 6))$.
$M$ is the set containing the entity profiles that refer to entities, while
$U$ is the set of entity profiles that don't refer entities.
Consequently, according to this model, M is the result of entity resolution.

A natural question is whether we should also include $U$ in the result.
Since $M$ will grow proportionally to the maximum number of entity references
extracted from a single data source and $U$ grows proportionally to
$A \times B$, we propose constructing $M$ and inferring $U$ based on $M$ for
practical reasons concerning memory space and processing time.

Another concern is whether the entity resolution result is uniquely determined
for a given input under this model.
There is evidence in literature that solutions under this model converge towards
a unique solution when given the same input data\cite{winkler2014matching}.

With the above definition for $Res$ and taking into account the limitations
expressed in the previous paragraph~(\ref{subsubsec:F-S Model Description}),
we observe that this model explains matching well.
Clustering entities only two at a time looks like a form of information loss.
As stated before, not being able to describe the result in terms of how well the
entity resolution task discovered transitive links between entity references
seems like a shortcoming.

\subsubsection{Specific Metrics}\label{subsubsec:F-S Quality Metrics}

Because we have a clear definition of the Type I and Type II errors, we also
have clear definitions of true and false positives and true and false negatives.
This allows us to use any of the familiar statistical metrics that rely on
these concepts.

In defining true/false positives/negatives we use the sets $M$ and $U$ as
they are defined in the original paper describing the F-S model.
Depending on where a pair (output by the entity resolution function) is
expected to be found, we define:

\begin{itemize}
    \item \textbf{true positives} as pairs predicted to be in $M$ that
    should be in $M$,
    \item \textbf{false positives}, or type I errors, as pairs predicted to
    be in $M$, but should be in $U$,
    \item \textbf{true negatives} as pairs predicted to be in $U$ that
    should be in $U$, and
    \item \textbf{false negatives}, or type II errors, as pairs predicted to
    be in $U$, but should be in $M$.
\end{itemize}

Quality metrics for this model compare a ground truth, consisting of ideal
tuple pairs, with the entity resolution task's output.
$M$ is the ground truth in this context.
Due to the impracticality of using $U$, our focus will be on concepts
related to $M$: true positives, false positives, and false negatives.

Several metrics based on these concepts exist, though the effectiveness of
some has been questioned\cite{Goga2015}.
With this in mind we choose three quality metrics that are widely used with
this model:

\begin{align}
Precision &= \frac{TP}{TP+FP}\nonumber \\
Recall &= \frac{TP}{TP+FN}\nonumber \\
F_1 &= 2 \cdot \frac{Precision \cdot Recall}{Precision+Recall}\nonumber
\end{align}

These metrics fit our quality metric definition from~\ref{sec:Entity Resolution Formalization}
and are defined on $Res^2$ with their result falling within the
$\left[0, 1\right] \subset \mathbb{R}$ closed interval.
Their input consists of two entity resolution results (represented as presented
previously) and their output is always a real number.

\textit{Precision} (or the positive predictive value) is defined as the
number of correct predictions that were made in relation to the total number
of predictions that were made.

\textit{Recall} (or sensitivity) is defined as the number of correct
predictions that were made in relation to the total number of positive
predictions that could have been made (which corresponds to the number of
items in the ground truth).

The \textit{$F_1$} score is the harmonic mean of the precision and the
recall and it is used to capture the tradeoff between precision and
recall\cite{hitesh2012}.

