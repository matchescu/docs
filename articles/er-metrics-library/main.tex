%! Author = andrei.olar@ubbcluj.ro
%! Date = 24.11.2023

% Preamble
\documentclass[11pt]{article}

% Packages
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{cite}
\usepackage{array}
\usepackage{graphicx}
\graphicspath{ {./img/} }

% Document
\begin{document}
    \theoremstyle{definition}
    \newtheorem{defn}{Definition}[section]
    
    \section{Abstract}\label{sec:abstract}


    \section{Introduction}\label{sec:introduction}
    Entity resolution involves determining whether two pieces of information
    represent the same real-world item.
    Some definitions view it as identifying and linking data from multiple
    sources\cite{Qia17}.
    However, it's argued that this identification and linking is a more
    specialized process\cite{Tal11}.
    Entity resolution, a broad issue, is known by various names including record
    linkage, data deduplication, merge-purge, named entity recognition, entity
    alignment, and entity matching.

    Entity resolution has many practical applications ranging from linking
    medical records to diagnosing diseases or ailments, to doing background
    checks for financial crime or to identifying plagiarism.
    Being a problem whose solutions could have a deep impact on both individual
    well-being and human society, we are obviously interested in understanding
    just how well entity resolution solutions fare.

    In the book ``Introduction to Information Retrieval'' we are acquainted with
    a number of ways in which the performance of information retrieval systems
    can be evaluated\cite{manning2008}.
    The metrics presented in this book revolve around the notion of relevant and
    irrelevant information that is retrieved by the system.
    The book also clarifies that the judgement around what is relevant or not is
    stipulated in a golden standard or a ground truth.
    Furthermore, this golden standard is dependant upon an information need.

    Entity resolution systems partly function as information retrieval systems,
    as they determine whether multiple data points refer to the same real-world
    entity.
    This capability to discern data identity is the fundamental information need
    of any entity resolution system.
    Is it therefore fitting to use information retrieval metrics for entity
    resolution?
    This seems to be the consensus drawn in the scientific literature as we
    shall see in the next section.

    The paper presents a library featuring implementations of various
    information retrieval metrics adapted for entity resolution.
    These metrics are designed to evaluate entity resolution functions, yet it's
    important to acknowledge the distinct mathematical models underlying entity
    resolution compared to information retrieval.
    The library sets itself apart by organizing metrics based on their
    compatibility with entity resolution models, influenced partly by the
    underlying differences in data structures that are characteristic to each
    model and partly by personal bias.

    \section{Related Work}\label{sec:related}

    Measuring entity resolution quality was a subject of interest ever since the
    first paper on the subject surfaced\cite{newcombe1959}.
    It speaks about accuracy and contamination similarly to how we describe true
    and false positives today.
    The fundamental theory of record linkage\cite{fs1969} offers a probabilistic
    approach to evaluating the success of an entity resolution task.
    It suggests methods to affect the results by selecting suitable thresholds
    for defining success and failure and weights for independent probability
    variables.
    The literature expands on these techniques in subsequent papers\cite{winkler1990}.
    Some of the useful entity resolution evaluation metrics include match
    accuracy, match rate\cite{jaro1989advances}, error rate estimation, rate
    of clerical disambiguation\cite{winkler1990} or relative distinguishing
    power of matching variables\cite{winkler2014}.
    A good amount of effort is spent on estimating and measuring the
    effectiveness of blocking techniques to reduce the input size of the
    evaluation data set\cite{winkler1990,jaro1989advances}.
    Measuring entity resolution performance was and remains a computationally
    intensive task.
    
    The concerns with using accuracy and match rate are voiced relatively
    recently\cite{Goga2015}.
    In recent years we see a shift towards metrics used in the related field of
    information retrieval.
    The probabilistic model for entity resolution aligns well with concepts such
    as true/false positives/negatives.
    Given the extensive history of using ground truths to assess entity
    resolution quality, most literature on this topic focuses on using
    statistical metrics similar to those in information retrieval
    evaluation\cite{manning2008}.

    Besides the original statistical model for entity resolution, other models
    have evolved from it or alongside it.
    The work of the InfoLab at Stanford on their Stanford Entity Resolution
    Framework\cite{Ben2009Swoosh} and that of the Center for Entity Resolution
    and Information Quality at the University of Arkansas in Little
    Rock\cite{tal2007algebraic} stand out.
    These models of entity resolution also proposed metrics for evaluating the
    quality of the entity resolution process\cite{Men10,Tal11}.

    Also related to this paper, there is ample coverage of the metrics used for
    entity resolution in syntheses on the subject\cite{vldb2010,hitesh2012}.
    More and more of the papers on the subject list metrics that measure aspects
    of clustering than statistical metrics.
    For example, variation of information\cite{Men10} or the Rand
    index\cite{tal2007algebraic} come up regularly when attempting to measure
    the performance of entity resolution tasks.

    Lastly, there are numerous systems that perform entity resolution available.
    Some of these systems include modules that can evaluate the performance of
    a particular entity resolution
    solution\cite{fever2009,magellan2020,oyster2012}.
    There are other libraries that some or all of the metrics that our library
    implements\cite{nmeth2020scipy,ereval}.

    \section{Discussion}\label{sec:discussion}

    From a broader perspective, the necessity for a specialized library
    dedicated to evaluating entity resolution metrics might seem redundant.
    Additionally, the focus on Python as the programming platform further
    questions the need for such a library.
    This skepticism is rooted in the expectation that Python, being a highly
    popular programming platform, should already offer high-quality, reusable
    tools available for a wide range of applications --- including evaluating
    entity resolution results.
    
    Upon closer examination of the tools available for evaluating entity
    resolution tasks, certain limitations in the existing assumptions become
    apparent.
    While there are numerous libraries offering packages for computing entity
    resolution metrics, using a general-purpose library like SciPy raises
    concerns about elegance and efficiency.
    This is particularly relevant when the sole requirement is to compute entity
    resolution metrics, and the additional features of a comprehensive library
    are unnecessary.
    The challenge of seamlessly integrating an entity resolution evaluation
    library becomes even more pronounced when attempting to use the ones
    packaged with established entity resolution
    systems\cite{oyster2012,jedai2017,deepm2020,magellan2020} or when attempting
    to evaluate all of those systems.
    
    Conversely, when specifically searching for libraries that offer entity
    resolution metrics, it becomes evident that some crucial metrics essential
    for effectively evaluating entity resolution tasks may be
    absent\cite{ereval}.

    Approaching the issue from a different angle, using metrics from a
    general-purpose algorithmic library like Scipy (specifically
    \texttt{scipy.metrics}) for entity resolution evaluation requires strict
    adherence to certain protocols.
    For example, to calculate the Rand index, data clusters must be mapped with
    labels, and these labels must be provided as input.
    While this might seem simple, the user-friendliness of such an approach is
    debatable.
    The complexity of adapting existing data and managing the necessary labels
    for the package could potentially rival the complexity of computing the Rand
    index itself, mooting the use of the package.

    In short, here are the reasons we chose to implement such a library:
    \begin{itemize}
    \item Architecturally, adhering to the principle of 'do one thing and do
    it well' is beneficial.
    This approach avoids the biases and dependencies of general-purpose
    libraries like SciPy, which can complicate integration into our
    custom-designed software.
    \item Historically, entity resolution has adapted evaluation techniques from
    statistics, information retrieval, and graph theory, tailoring these methods
    to suit its specific needs.
    It seems desirable to standardize the way in which we use them.
    \item Currently, there appears to be no implementation that consolidates all
    the metrics useful for entity resolution evaluation, as identified in
    scientific literature, into a single, cohesive unit.
    \item Our work has a significant component of evaluating entity resolution
    outcomes.
    \end{itemize}

    The author has begun developing a new library dedicated to evaluating entity
    resolution tasks to aid them in their own work.
    They propose that using mathematical models specific to entity resolution is
    the best approach for guiding the library's design.
    Since each model significantly impacts the data structures used in
    evaluation, the library's functions are categorized based on the type of
    input they support and, implicitly, by the mathematical model they align
    with.

    There are a couple of important assumptions that the library makes,
    regardless of the entity resolution model.
    One such assumption is that there exists a ground truth\cite{manning2008} to
    compare the entity resolution outcome against.
    The other assumption is that the ground truth and the entity resolution
    result are both expressed using the same model.

    \section{Mathematical Models}\label{sec:models}
    \subsection{Fellegi-Sunter Model}
    In the late 1960s Ivan Fellegi and Alan Sunter wrote the seminal
    paper\cite{fs1969} for what they called record linkage and what would later
    become known as entity resolution.
    To this day, their mathematical model based on probability theory is the
    most popular way of formalizing the entity resolution problem.
    In this mathematical model, entity resolution is a function that aids in 
    probabilistic decision making.
    
    In this model of entity resolution, the process primarily involves comparing
    data from two sources.
    The essential step is matching two items --- one from each source, after
    which a decision is made to categorize the match as a `link', `non-link', or
    `possible link'.
    Consequently, any matching algorithm under this model typically returns
    pairs of items from the original data sources, each tagged as one of these
    categories.
    However, in practical applications today, this process is often simplified
    to just returning a list of pairs labeled as `links'.

    This intuitive explanation gives us the structure of the input we can expect
    when we use the Fellegi-Sunter entity resolution model: a list of pairs.
    So, the metrics that are implemented with this statistical model of entity
    resolution in mind will only accept lists of pairs as the ground truth and
    entity resolution result.

    \subsection{Algebraic Model}
    The algebraic model for entity resolution, initially conceived for assessing
    information quality in large datasets\cite{tal2007algebraic}, was later
    refined to describe the entity resolution process itself\cite{Tal11}.
    This model treats entity resolution as an algebraic equivalence relation
    over a given input set, which can include data from as many original sources
    as necessary.
    
    The unique aspect of this model lies in the characteristics of equivalence
    relations\cite{halmos1960naive}.
    These relations create partitions over the input set, with each partition
    component equivalent to an equivalence class of the relation\cite{Tal11}.
    Conversely, a partition over a set can also induce an equivalence relation.

    With this in mind, evaluating the outcome of an entity resolution task
    becomes as easy as comparing two partitions: the partition that induces the
    ideal equivalence relation (the gold standard or ground truth) to the
    partition that is produced by the entity resolution task.
    The library supports a few metrics for comparing partitions, all of which
    expect that a partition is represented as a list of sets.

    \section{Supported Metrics}\label{sec:metrics}
    
    \subsection{Statistical Metrics}
    Statistical quality metrics, extensively detailed in the
    literature\cite{manning2008,hitesh2012}, are the most common method for
    measuring entity resolution performance as evidentiated by their almost
    ubiquitous usage\cite{fever2009,Goga2015,deepm2020,eager2021}.
    The Fellegi-Sunter model provides clear definitions of Type I and Type II
    errors\cite{winkler1990}, which in turn clarify the concepts of true
    positives, true negatives, false positives, and false negatives.
    Understanding these concepts necessitates referencing the $M$ (matches) and
    $U$ (non-matches) sets as defined in the seminal paper on the statistical
    model.

    Depending on the expected location of a pair produced by the entity
    resolution function, we define:

    \begin{itemize}
        \item \textbf{true positives} as pairs predicted to be in $M$ that
        should be in $M$,
        \item \textbf{false positives}, or type I errors, as pairs predicted to
        be in $M$, but should be in $U$,
        \item \textbf{true negatives} as pairs predicted to be in $U$ that
        should be in $U$, and
        \item \textbf{false negatives}, or type II errors, as pairs predicted to
        be in $U$, but should be in $M$.
    \end{itemize}

    Several metrics based on these concepts exist, though the effectiveness of
    some has been questioned\cite{Goga2015}.
    With this in mind we finally define the three quality metrics that are
    supported by our library:

    \begin{align}
    Precision&=\frac{true positives}{true positives + false positives} \\
    Recall&=\frac{true positives}{true positives + false negatives} \\
    F_1 Score&=2 \cdot \frac{Precision \cdot Recall}{Precision+Recall}
    \end{align}

    \textit{Precision} (or the positive predictive value) is defined as the
    number of correct predictions that were made in relation to the total number
    of predictions that were made.
    \textit{Recall} (or sensitivity) is defined as the number of correct
    predictions that were made in relation to the total number of positive
    predictions that could have been made (which corresponds to the number of
    items in the ground truth).
    The \textit{$F_1$} score is the harmonic mean of the precision and the
    recall and it is used to capture the tradeoff between precision and
    recall\cite{hitesh2012}.

    \subsection{Algebraic Metrics}
    While commonly known as `cluster metrics'\cite{rand1971,hitesh2012} or
    `pairwise metrics'\cite{hitesh2012,Men10}, we find that they are much better
    described as `algebraic' because the principle of operation rests on an
    algebraic foundation for all of these metrics.
    In fact, most of the ones that are implemented by the library are an
    exercise in using operations on sets, while the rest focus on matrix
    operations with a dash of combinatorics:
    \begin{itemize}
        \item Pairwise metrics (precision, recall and F-measure)\cite{Men10,hitesh2012}
        \item Cluster metrics (precision, recall and F-measure)\cite{huang2006efficient,hitesh2012}
        \item Talburt-Wang Index\cite{tal2007algebraic}
        \item Rand\cite{rand1971} and Adjusted Rand Index\cite{adjrand1985}
    \end{itemize}
    
    The Rand index is one of the first metrics used to compare the similarity
    between two different data clusterings.
    It quantifies the agreement or disagreement between these clusterings by
    considering pairs of elements.
        \[Rand Index = \frac{(a + b)}{{n \choose 2}}\]
    The main components of the Rand index are as follows:
    \begin{itemize}
    \item a: Represents the number of times a pair of elements belongs to the
        same cluster across both clustering methods.
    \item b: Represents the number of times a pair of elements belongs to
        different clusters across both clustering methods.
    \item $n \choose 2$: Denotes the number of unordered pairs in a set of n
        elements.
    \end{itemize}
    
    The Rand index always takes values in the $\left[0, 1\right)$ interval.

    A variation on the Rand Index is the Adjusted Rand Index for chance grouping
    of elements.
    It accounts for agreements between data clusterings that occur due to
    chance\cite{adjrand2001}.
    The Adjusted Rand Index is calculated by using the following formula:
    
    \[ ARI = \frac{RandIndex - E}{\max(RandIndex) - E}, \]

    where $E$ is the expected value of the RandIndex.
    The Adjusted Rand index is valued in the interval $\left[-1, 1\right]$.
    For a comprehensive understanding of the Adjusted Rand Index and its
    calculation, we recommend consulting the detailed and informative work
    presented in the study by Warrens and van der Hoef (2022) on the
    subject\cite{warrens2022understanding}.

    For both of these indexes, higher scores indicate a closer alignment between
    the compared partitions.

    A metric related to the Rand Index is the Talburt-Wang Index which counts
    the number of overlapping subsets of two partitions over the same input set.
    Assuming $A$ and $B$ are two partitions over the same input set of elements,
    the Talburt-Wang Index is given by the formula:

    \[ \varDelta(A, B) = \frac{|A|\cdot|B|}{\varPhi{\left(A, B\right)}^2}\textrm{, where}\]
    \[ \varPhi(A, B) = \sum_{i=1}^{|A|}\{B_j \in B | B_j \cap A_i \neq \emptyset \} \]

    This metric approximates the Rand Index without requiring the expensive
    counting of true positives, false positives, true negatives or false
    negatives.
    
    Besides the Rand Index, other popular metrics that can be used for comparing
    partitions are the pairwise precision, pairwise recall and their harmonic
    mean (commonly referred to as the pairwise F measure).
    If we have two sets $X$ and $Y$, the pairwise precision is given by the
    fraction of pairs that are in both sets over the total amount of pairs of
    the reference set\cite{hitesh2012}.
    \[ Pair Precision(X, Y) = \frac{|{Pairs(X)}\cap{Pairs(Y)}|}{|Pairs(X)|} \]
    
    The pairwise recall is given by the fraction of pairs that are in both sets
    over the number of pairs in the challenger set\cite{hitesh2012}.
    \[ Pair Recall(X, Y) = \frac{|{Pairs(X)}\cap{Pairs(Y)}|}{|Pairs(Y)|} \]

    The pairwise F-measure is given by the harmonic mean of the pairwise
    precision and pairwise recall.

    \[ Pair F = \frac{2 \cdot Pair Precision \cdot Pair Recall}{Pair Precision + Pair Recall} \]

    The library computes these metrics on partitions by iterating through the
    subsets of each partition and extracting pairs of elements from each subset.
    
    Finally, the library supports computing a few `cluster measures'\cite{hitesh2012}.
    cluster precision is the ratio of the number of completely correct
    clusters to the total number of clusters retrieved, whereas cluster recall
    is the portion of true clusters retrieved\cite{huang2006efficient}.
    Their harmonic mean is typically called the cluster F-measure.
    Given two partitions $X$ and $Y$, the cluster measures are given by the
    following formulae:

    \begin{align*}
        Cluster Precision(X, Y) &= \frac{|{X}\cap{Y}|}{|X|}\\
        Cluster Recall(X, Y) &= \frac{|{X}\cap{Y}|}{|Y|}\\
        Cluster F &= \frac{2\cdot{Cluster Precision}\cdot{Cluster Recall}}{Cluster Precision + Cluster Recall}
    \end{align*}

    \section{Technology}\label{sec:tech}

    The library is implemented using the Python programming language\cite{python}.
    It makes use of the Numpy library\cite{harris2020numpy} to accelerate
    the expensive computations required by some of the implemented metrics.

    The technology choice deserves special attention because of an uncommon
    reason.
    Normally, a partition $P$ over a set $X$ is defined as a set of
    non-overlapping subsets containing all the elements from $X$, that is:
    \[
        P = \{ C_1, C_2, \ldots, C_n | \bigcap\limits^n_{i=1}C_i=\emptyset \land \bigcup\limits^n_{i=1}C_i=X \}
    \]

    The library implements metrics that compare partitions, but it requires the
    partition to be specified as a \textit{list} of sets, instead of a set of
    sets.
    The reason behind this choice is purely technological and fundamentally lies
    with the choices made by the Python core team:
    \begin{enumerate}
        \item allowing the set data structure to be changed in memory over time,
        \item not providing a hashing function for the set data structure.
    \end{enumerate}

    The two choices essentially negate using sets within sets.
    The option exists to use a data structure called a `frozen' set, but that
    contract seemed too restrictive to impose on the library's consumers.

    \section{Future Work}\label{sec:conclusions}

    \subsection{Missing Metrics}

    The library of entity resolution metrics is far from complete.


    \bibliographystyle{plain} % We choose the "plain" reference style
    \bibliography{er-general,er-related-work,er-additional-references,er-software}
\end{document}
