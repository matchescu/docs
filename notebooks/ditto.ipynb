{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9239db8823ba023",
   "metadata": {},
   "source": [
    "# Using Pre-trained Transformers for Matching\n",
    "\n",
    "Start out by declaring a few constants."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T18:05:00.964683Z",
     "start_time": "2025-03-26T18:05:00.959998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import polars as pl\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "DATADIR = os.path.abspath(\"../../data\")\n",
    "MODELDIR = os.path.abspath(\"../../models\")\n",
    "\n",
    "BERT_MODEL_NAME = \"roberta-base\"\n",
    "LEFT_CSV_PATH = os.path.join(DATADIR, \"abt-buy\", \"Abt.csv\")\n",
    "RIGHT_CSV_PATH = os.path.join(DATADIR, \"abt-buy\", \"Buy.csv\")\n",
    "GROUND_TRUTH_PATH = os.path.join(DATADIR, \"abt-buy\", \"abt_buy_perfectMapping.csv\")"
   ],
   "id": "17fbc732bd2e7b2f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Next, extract entity references and the ground truth from an existing CSV dataset.\n",
    "The entity references are stored in an \"ID\" table."
   ],
   "id": "df91db57dcd9d9b1"
  },
  {
   "cell_type": "code",
   "id": "b9741bb63d0c5d05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T18:05:01.015979Z",
     "start_time": "2025-03-26T18:05:00.980357Z"
    }
   },
   "source": [
    "from functools import partial\n",
    "from matchescu.extraction import (\n",
    "    RecordExtraction,\n",
    "    single_record,\n",
    "    Traits,\n",
    ")\n",
    "from matchescu.data_sources import CsvDataSource\n",
    "from matchescu.reference_store.id_table import InMemoryIdTable\n",
    "from matchescu.typing import EntityReferenceIdentifier\n",
    "\n",
    "abt_traits = list(Traits().string([\"name\", \"description\"]).currency([\"price\"]))\n",
    "abt = CsvDataSource(LEFT_CSV_PATH, traits=abt_traits).read()\n",
    "buy_traits = list(\n",
    "    Traits().string([\"name\", \"description\", \"manufacturer\"]).currency([\"price\"])\n",
    ")\n",
    "buy = CsvDataSource(RIGHT_CSV_PATH, traits=buy_traits).read()\n",
    "\n",
    "gt = set(\n",
    "    (\n",
    "        EntityReferenceIdentifier(id_abt, abt.name),\n",
    "        EntityReferenceIdentifier(id_buy, buy.name),\n",
    "    )\n",
    "    for id_abt, id_buy in pl.read_csv(\n",
    "        os.path.join(DATADIR, \"abt-buy\", \"abt_buy_perfectMapping.csv\"),\n",
    "        ignore_errors=True,\n",
    "    ).iter_rows()\n",
    ")\n",
    "\n",
    "\n",
    "def _id(records, source):\n",
    "    return EntityReferenceIdentifier(records[0][\"id\"], source)\n",
    "\n",
    "\n",
    "def load_data_source(data_source: CsvDataSource) -> None:\n",
    "    extract_references = RecordExtraction(\n",
    "        data_source, partial(_id, source=data_source.name), single_record\n",
    "    )\n",
    "    for ref in extract_references():\n",
    "        id_table.put(ref)\n",
    "\n",
    "\n",
    "id_table = InMemoryIdTable()\n",
    "load_data_source(abt)\n",
    "load_data_source(buy)\n",
    "original_comparison_space_size = len(abt) * len(buy)\n",
    "print(\n",
    "    f\"total entity references: {len(id_table)}, original_comparison_space_size: {original_comparison_space_size}\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total entity references: 2173, original_comparison_space_size: 1180452\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "d001811f44c60ae3",
   "metadata": {},
   "source": [
    "Next up, we create the comparison space.\n",
    "A __binary__ comparison space is a list of pairs of entity reference identifiers.\n",
    "The entity references identified in this way are deemed more suitable than others to match.\n",
    "The comparison space is generated through blocking and filtering."
   ]
  },
  {
   "cell_type": "code",
   "id": "970ed70d018355a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T18:05:03.992687Z",
     "start_time": "2025-03-26T18:05:01.026608Z"
    }
   },
   "source": [
    "from matchescu.comparison_filtering import is_cross_source_comparison\n",
    "from matchescu.blocking import TfIdfBlocker, SortedNeighborhoodBlocker, LSHBlocker\n",
    "from matchescu.csg import BinaryComparisonSpaceGenerator, BinaryComparisonSpaceEvaluator\n",
    "\n",
    "csg = (\n",
    "    BinaryComparisonSpaceGenerator()\n",
    "    .add_blocker(TfIdfBlocker(id_table, 0.25))\n",
    "    .add_blocker(SortedNeighborhoodBlocker(id_table, 12))\n",
    "    .add_blocker(LSHBlocker(id_table, 0.25))\n",
    "    .add_filter(is_cross_source_comparison)\n",
    ")\n",
    "comparison_space = csg()\n",
    "eval_cs = BinaryComparisonSpaceEvaluator(gt, original_comparison_space_size)\n",
    "metrics = eval_cs(comparison_space)\n",
    "print(metrics)\n",
    "print(\"comparison space size:\", len(comparison_space))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlockingMetrics(pair_completeness=0.5943482224247949, pair_quality=0.034706696476099225, reduction_ratio=0.9840857569812241)\n",
      "comparison space size: 18786\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "26298c7a5a35eea0",
   "metadata": {},
   "source": [
    "Next, we need to load a pretrained matcher. This requires training a model.\n",
    "We're using the [Ditto classifier](https://github.com/megagonlabs/ditto/tree/master/ditto_light).\n",
    "To train Ditto using a BERT model, see the `matchescu.matching.ml.ditto.train` module."
   ]
  },
  {
   "cell_type": "code",
   "id": "839f3e67bfdbc056",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T18:05:06.098712Z",
     "start_time": "2025-03-26T18:05:04.001360Z"
    }
   },
   "source": [
    "from matchescu.matching.ml.ditto import DittoSimilarity\n",
    "\n",
    "matcher = DittoSimilarity(\n",
    "    AutoTokenizer.from_pretrained(BERT_MODEL_NAME),\n",
    "    model_dir=MODELDIR,\n",
    "    left_cols=(\"name\", \"description\", \"price\"),\n",
    "    right_cols=(\"name\", \"description\", \"manufacturer\", \"price\"),\n",
    ")\n",
    "matcher.load_pretrained(BERT_MODEL_NAME)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "26f4c8842c3d3685",
   "metadata": {},
   "source": "It's time to run the matcher. We need to compute and store the matcher's predictions separately in order to evaluate them."
  },
  {
   "cell_type": "code",
   "id": "459dcdd56883c2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T18:19:00.223718Z",
     "start_time": "2025-03-26T18:05:06.106301Z"
    }
   },
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# comparison space ground truth\n",
    "csgt = [int(pair in gt) for pair in comparison_space]\n",
    "refs = list(map(tuple, map(id_table.get_all, comparison_space)))\n",
    "\n",
    "# run matching algorithm on comparison space\n",
    "match_scores = {(x, y): matcher(x, y) for x, y in refs}\n",
    "pred = [int(v > matcher.match_threshold) for v in match_scores.values()]\n",
    "print(f\"ground truth size: {len(csgt)}, prediction size: {len(pred)}\")\n",
    "\n",
    "# evaluate matching performance\n",
    "print(\n",
    "    \"precision: %.2f, recall: %.2f, F1: %.2f\"\n",
    "    % (\n",
    "        precision_score(csgt, pred),\n",
    "        recall_score(csgt, pred),\n",
    "        f1_score(csgt, pred),\n",
    "    )\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth size: 18786, prediction size: 18786\n",
      "precision: 0.70, recall: 0.81, F1: 0.75\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Finally, we construct the similarity graph.\n",
    "Since not all similarity computations are symmetric (e.g neural networks with asymmetric activation functions like ReLU) => `matcher(a, b) != matcher(b, a)`.\n",
    "That means that the similarity graph is a directed graph."
   ],
   "id": "dd79cffb65d4e16e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T18:19:00.275461Z",
     "start_time": "2025-03-26T18:19:00.236129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matchescu.references import EntityReference\n",
    "from functools import reduce\n",
    "from itertools import starmap\n",
    "from matchescu.similarity import SimilarityGraph\n",
    "from pyresolvemetrics import precision, recall, f1\n",
    "\n",
    "\n",
    "# at runtime, the evaluated matcher is used directly instead of this stub\n",
    "class MatcherStub:\n",
    "    def __init__(self, scores: dict, threshold: float):\n",
    "        self.__threshold = threshold\n",
    "        self.__match_scores = scores\n",
    "\n",
    "    @property\n",
    "    def non_match_threshold(self) -> float:\n",
    "        return self.__threshold\n",
    "\n",
    "    @property\n",
    "    def match_threshold(self) -> float:\n",
    "        return self.__threshold\n",
    "\n",
    "    def __call__(self, a: EntityReference, b: EntityReference) -> float:\n",
    "        return self.__match_scores[(a, b)]\n",
    "\n",
    "\n",
    "# effectively make the match/non-match choice binary\n",
    "simg = reduce(\n",
    "    lambda x, pair: x.add(*pair),\n",
    "    refs,\n",
    "    SimilarityGraph(\n",
    "        MatcherStub(match_scores, matcher.match_threshold),\n",
    "        matcher.match_threshold,\n",
    "        matcher.match_threshold\n",
    "    ),\n",
    ")\n",
    "print(repr(simg))\n",
    "# the scores should be the same as the previous cell scores\n",
    "cs_true_matches = set(comparison_space) & set(gt)\n",
    "sim_graph_matches = frozenset(starmap(lambda x, y: (x.id, y.id), simg.matches()))\n",
    "print(\n",
    "    \"precision: %.2f, recall: %.2f, F1: %.2f\"\n",
    "    % (\n",
    "        precision(cs_true_matches, sim_graph_matches),\n",
    "        recall(cs_true_matches, sim_graph_matches),\n",
    "        f1(cs_true_matches, sim_graph_matches),\n",
    "    )\n",
    ")"
   ],
   "id": "ac4da705115d12d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimilarityGraph(nodes=2172, edges=755, match=755, non_match=18031, maybe=0)\n",
      "precision: 0.70, recall: 0.81, F1: 0.75\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T18:19:15.885060Z",
     "start_time": "2025-03-26T18:19:14.721869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyresolvemetrics import cluster_precision, cluster_recall, cluster_comparison_measure, rand_index, \\\n",
    "    adjusted_rand_index, twi, pair_precision, pair_recall, pair_comparison_measure\n",
    "from matchescu.clustering import EquivalenceClassPartitioner\n",
    "\n",
    "\n",
    "unique_ids = list(map(lambda r: r.id, id_table))\n",
    "partitioner = EquivalenceClassPartitioner(unique_ids)\n",
    "gt_clusters = partitioner(cs_true_matches)\n",
    "result_clusters = partitioner(sim_graph_matches)\n",
    "\n",
    "print(\n",
    "    \"pair precision: %.2f, pair recall: %.2f, pair F-measure: %.2f\"\n",
    "    % (\n",
    "        pair_precision(gt_clusters, result_clusters),\n",
    "        pair_recall(gt_clusters, result_clusters),\n",
    "        pair_comparison_measure(gt_clusters, result_clusters),\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"cluster precision: %.2f, cluster recall: %.2f, cluster F-measure: %.2f\"\n",
    "    % (\n",
    "        cluster_precision(gt_clusters, result_clusters),\n",
    "        cluster_recall(gt_clusters, result_clusters),\n",
    "        cluster_comparison_measure(gt_clusters, result_clusters),\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"Rand index: %.2f, adjusted Rand index: %.2f, Talburt-Wang index: %.2f\"\n",
    "    % (\n",
    "        rand_index(gt_clusters, result_clusters),\n",
    "        adjusted_rand_index(gt_clusters, result_clusters),\n",
    "        twi(gt_clusters, result_clusters),\n",
    "    )\n",
    ")"
   ],
   "id": "1f2377af0919b090",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair precision: 0.65, pair recall: 0.98, pair F-measure: 0.78\n",
      "cluster precision: 0.95, cluster recall: 0.88, cluster F-measure: 0.91\n",
      "Rand index: 1.00, adjusted Rand index: 0.80, Talburt-Wang index: 0.93\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T18:19:01.457411Z",
     "start_time": "2025-03-26T18:19:01.456026Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "da6b065ef3aeabb2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
